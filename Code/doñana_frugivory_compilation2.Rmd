---
title: "Doñana_frugyvory_camptrap"
author: "PVA"
date: "6/7/2023"
output: html_document
---
The following scripts compile several datasets for Doñana-camtrap-frugivory data paper.

We will merge and standardise data from Sumhal yr1/yr2 & Corema1 & Juniperus phoenicea (Islas phD thesis) & Pistacia lentiscus (Quinteros phD thesis). 

The script has 4 main parts: 

1. First I will prepare sampling effort and coordinates to include in the final dataset.

2. Data unification - We give consistency to all the datasets by correcting inconsistencies, filtering results including new variables like sampling effort or coordinates and video duration extracted from files metadata. Resulting videos are collapsed to 5 minutes intervals considered as independent events. 

3. Data merging - Here we integrate (merge) all the datasets into one final df while we correct some inconsistencies and introduce samplig effort at species and individual level

4. Preliminary analysis - Here we present data summaries from fito and zoocentric perspectives, basic trophic webs, and basic preliminary analysis to show possible uses of the dataset. We plot charts with number of recordings, sampling efforts, etc. and present first attempts to activity foraging patterns of some animal species as examples.   

Libraries
```{r}
library(dplyr)
library(stringr)
library(tidyr)
library(bipartite)
library(ggplot2)
library(tidyverse)

#For activity
library(activity)
library(camtrapR)
library(overlap)
library(astroFns)
```

# 1. EFFORT & COORDINATES
## Load sampling effort and coordinates
```{r}
#Sampling effort at individual and species level
effort_individual <- read.csv("/Users/Pablo/Documents/GitHub/donana-frugivory-camtrap/Metadata/Sampling_effort/effort_individual_level.csv")

effort_species <- read.csv("/Users/Pablo/Documents/GitHub/donana-frugivory-camtrap/Metadata/Sampling_effort/effort_species.csv")

#Coordinates
coord <- read.csv("/Users/Pablo/Documents/GitHub/donana-frugivory-camtrap/Metadata/Coordinates/plant_coordinates.csv")
  
```



# 2. DATA UNIFICATION
## 2.1 SUMHAL YR1
### 1. Load data  
    Read the resulting CSV from Time-lapse visualization (after running the AI process). 
    Note that for fieldwork-YR1 there are 4 result csvs (d1 to d4).
```{r}
d1 <- read.csv("/Users/Pablo/Documents/GitHub/donana-frugivory-camtrap/Metadata/raw_data/Timelapse_results_Yr1/Phototrapping_data1.csv") %>%
  select(File, RelativePath, DateTime, Sp1, Behaviour, Count1, Sp2, Behaviour2, Count2, Sp3, Count3, Favourite, Obs)

d2 <- read.csv("/Users/Pablo/Documents/GitHub/donana-frugivory-camtrap/Metadata/raw_data/Timelapse_results_Yr1/Phototrapping_data2.csv") %>% # (1) need to change "Pyrus_r6" to "Pyrus"
  select(File, RelativePath, DateTime, Sp1, Behaviour, Count1, Sp2, Behaviour2, Count2, Sp3, Count3, Favourite, Obs)

d3 <- read.csv("/Users/Pablo/Documents/GitHub/donana-frugivory-camtrap/Metadata/raw_data/Timelapse_results_Yr1/Phototrapping_data3_Pyrus.csv") %>% # (2) need to add Pyrus to the file path
  select(File, RelativePath, DateTime, Sp1, Behaviour, Count1, Sp2, Behaviour2, Count2, Sp3, Count3, Favourite, Obs)

d4 <- read.csv("/Users/Pablo/Documents/GitHub/donana-frugivory-camtrap/Metadata/raw_data/Timelapse_results_Yr1/Phototrapping_data4_MD_last_run.csv") %>% #Need to revise all the steps, because don´t know the triquiñuels
  select(File, RelativePath, DateTime, Sp1, Behaviour, Count1, Sp2, Behaviour2, Count2, Sp3, Count3, Favourite, Obs)

d5 <- read.csv("/Users/Pablo/Documents/GitHub/donana-frugivory-camtrap/Metadata/raw_data/Timelapse_results_Yr1/Phototrapping_data5_joxy_yr1_revised_c05.csv") %>%
  select(File, RelativePath, DateTime, Sp1, Behaviour, Count1, Sp2, Behaviour2, Count2, Sp3, Count3, Favourite, Obs)
```

### 2. Correct INCONSISTENCIES & MERGE                          - 29,123 rows
    To create the Data Base from time-lapse outputs (csv´s) we need to correct some inconsistencies in     the paths that are coming from different AI-MD runs or inconsistent path/file names.
    "data_yr1" contains the unified visualised datasets (Timelapse results) with consistent relative paths. 
```{r}
# d1 Change for path consistency. Create new path column (w/o inconsistencies)
d1 <- d1 %>%
  mutate(new_path = str_replace_all(RelativePath, "Review", "Rev")) %>%
  mutate(new_path = str_to_lower(new_path)) %>%
  mutate(new_path = str_replace_all(new_path, "rev_", "rev"))

# d2 Change for path consistency (new path column) 
d2 <- d2 %>%
  mutate(new_path = str_replace_all(RelativePath, "Rev_", "Rev")) %>%
   mutate(new_path = str_to_lower(new_path)) %>%
   mutate(new_path = str_replace_all(new_path, "pyrus_r6", "pyrus"))

# d3 Change for path consistency (new path column) (2) add "Pyrus" to the path
d3 <- d3 %>%
  mutate(new_path = file.path ("Pyrus\\", RelativePath)) %>%
  mutate(new_path = str_remove_all(new_path, "/")) %>%
  mutate(new_path = str_replace_all(new_path, "Rev_", "rev")) %>%
  mutate(new_path = str_to_lower(new_path))

# d4 Change for path consistency (new path column) 
d4 <- d4 %>%
  mutate(new_path = str_replace_all(RelativePath, "Rev_", "Rev")) %>%
  mutate(new_path = str_to_lower(new_path))

# d5 Change for path consistency (new path column) 
d5 <- d5 %>%
  mutate(new_path = str_to_lower(RelativePath))

#Merge the 4 datasets 
data_yr1 <- rbind(d1,d2,d3,d4,d5) 
```

### 3. Create PLANT column & FILTER non animals                 - 11,709 rows
  Introduce Plant name, eliminate car and person and empty videos. We also correct mistaken names and change counts from 0 (default to 1)
  "dat_yr1" contains results only for animals (wild and domestic) and corrected animal names. 
```{r}
#Create a new column with the plant species (from the file path)
plant <- word(data_yr1$new_path, 1, sep = fixed("\\"))

#Merge them to the dataset
data_yr1 <- cbind(data_yr1,plant)    # 29,123 rows

# NAME CORRECTIONS AND WILD ANIMAL SELECTION #
#Eliminate empty videos, car, person, horse and cows   - 11,709 rows
dat_yr1 <- data_yr1 %>%
  filter(Sp1 != "") %>% 
  filter(Sp1 != "car") %>% 
  filter(Sp1 != "person") %>%
  filter(Sp1 != "empty")

#Correct mistaken names in Sp1
dat_yr1$Sp1 [dat_yr1$Sp1 == "ce" ] <- "cervus elaphus" 
dat_yr1$Sp1 [dat_yr1$Sp1 == "cervu`s elaphus" ] <- "cervus elaphus"
dat_yr1$Sp1 [dat_yr1$Sp1 == "at" ] <- "athene noctua" 
dat_yr1$Sp1 [dat_yr1$Sp1 == "frigilla coelebs" ] <- "fringilia coelebs"
dat_yr1$Sp1 [dat_yr1$Sp1 == "saxicola rubicoola" ] <- "saxicola rubicola" 
dat_yr1$Sp1 [dat_yr1$Sp1 == "se" ] <- "serinus serinus" 
dat_yr1$Sp1 [dat_yr1$Sp1 == "equus cabalus" ] <- "equus caballus" 
dat_yr1$Sp1 [dat_yr1$Sp1 == "orictolagus cuniculus" ] <- "oryctolagus cuniculus"

#Correct mistaken names in Sp2
dat_yr1$Sp2 [dat_yr1$Sp2 == "eri" ] <- "erithacus rubecula"

#Change Count to the number of individuals  
dat_yr1$Count1 [dat_yr1$Count1 == 0 ] <- 1
dat_yr1$Count2 <- ifelse(dat_yr1$Sp2 != "", 1, 0)

```

### 4. Include VIDEO DURATION                                   - 11,709 rows
  Extracts the video duration from external metadata files (videos that are placed in G-Raid-1 called "G-RAID") to include a "duration" column that will represent the interaction intensity.
```{r}
#Just in case dont want to run the time demanding code.
dat_yr1_duration <- read.csv2("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/duration/dat_yr1_duration.csv", sep=",")

#--------------------- SKIP ---------------------------------------- 
# Get a list of ALL files in GRAID 
dir <- list.dirs ("/Volumes/G-RAID/SUMHAL") #List all directories in the Hard Drive
fil <- list.files(dir, full.names = T) #List the files
fil_clean <- fil[!file.info(fil)$isdir] #List of files without empty folders 
  #write.csv(fil_clean, "/Users/PV/video_file_list.csv", sep = ",")

# Get a list of files with eating events (eating videos)  
files_dat <- str_c(dat_yr1$RelativePath, dat_yr1$File, sep ="/")  #List of videos with foraging behavior 
files_dat_path <- file.path("/Volumes/G-RAID/SUMHAL", files_dat) #Add file path

# Correct some path syntax mistakes (detected by setdiff function, see below)   
fil_eat_clean <- files_dat_path %>%
  str_replace_all("\\\\", "/") %>%
  str_replace_all("Pyrus_r6", "Pyrus") %>%  # correct errors in 3 paths
  str_replace_all("/Volumes/G-RAID/SUMHAL/Rev", "/Volumes/G-RAID/SUMHAL/Pyrus/Rev") %>%    
  str_replace_all("/Volumes/G-RAID/SUMHAL/Pyrus/Rev_5", "/Volumes/G-RAID/SUMHAL/Pyrus/Rev_5_20211210")   

# Compare eating list to the entire file list (all files in GRAID) to detect missmatches  
fil_lost <- setdiff(fil_eat_clean, fil_clean) #What is in my_list_2 that is not in my_list_1?
length(fil_lost)

# Extract video duration from eating list
info <- lapply(fil_eat_clean, av_media_info)  #apply info extraction function to video list (Time demanding)
duration <- sapply(info, function(x){as.numeric(x[1])}) #Extract only video duration (position 1)

  #write.csv(info, "/Users/PV/Desktop/video_info_yr1.csv", row.names = FALSE)

# Join duration to database  
dat_yr1_duration <- cbind(dat_yr1, duration)

   #write.csv(dat_yr1_duration,"/Users/PV/Desktop/dat_yr1_duration.csv")
```

### 5. Create new columns & SELECT variables & FILTER BEHAVIOUR - 6,053 rows
      Create Plant_ID & Revision_id columns. Create a new df == new_dat_yr1 filtered by behavir 
```{r}
## CLEAN THE DATA (Video level == Data non collapsed to 5 min)
#Split path in different columns  
clean <- data.frame(str_split_fixed(dat_yr1_duration$new_path, pattern ="\\\\", 3)) 
colnames(clean) <- c("Plant_sp", "Rev", "Plant_ID")
unique(clean$Rev)

#Plant_ID
Plant_ID <- clean %>%
  select (Plant_ID) %>%
  mutate (Plant_ID = str_remove_all (Plant_ID, "_")) %>%
  mutate (Plant_ID = str_sub (Plant_ID, start = 1L, end = 7)) %>%
  mutate (Plant_ID = str_replace_all(Plant_ID, "a05\\\\100", "a05")) %>%   #Change the name for this Corema album that was wrong!! note the \\\\ not \\
  mutate (Plant_ID = str_replace_all(Plant_ID, "pbou121", "pbou012")) 

#Revision_ID
Rev <- data.frame (str_sub(clean$Rev, start = 1L, end = 5)) %>%
  rename(Rev = str_sub.clean.Rev..start...1L..end...5.) %>%
  mutate(Rev = str_remove_all(Rev, "_")) %>%
  mutate(Rev = str_replace_all(Rev, "rev", "rev0")) %>%  #Introduce 01, 02, 03 ... for consistency when joining with sampling effort
  mutate(Rev = str_replace_all(Rev, "rev010", "rev10")) %>%
  mutate(Rev = str_replace_all(Rev, "rev011", "rev11")) 

#Bind columns created above & select variables & FILTER FORAGING BEHAVIOURS
new_dat_yr1 <- dat_yr1_duration %>%
  select(File, RelativePath, new_path, DateTime, Sp1, Behaviour, Count1, Sp2, Behaviour2, Count2, Sp3, plant, duration, Favourite, Obs) %>%
  cbind(Plant_ID, Rev) %>%
  mutate(ID = str_c (Plant_ID, Rev, sep="_")) %>%
  filter(Behaviour == "eating"|Behaviour == "probably eating"| Behaviour == "searching for food") #Filter only eating (for simplifying later merging with sampling effort and location)

```

### 6. Introduce SAMPLING EFFORT                                - 6,053 rows
  There is a code to automatize the extraction of video file information regarding to Sampling effort at deployment and upper level. The script is called videofile_batch_data_extraction and creates the video_yr1.csv where SAMPLING EFFORT for EACH INDIVIDUAL PLANT AND REVISION is obtained for the entire set of videos in Sumhal_Yr1 hard drive.  
```{r}
#In case want to skip all this code
dat_yr1_effort <- read.csv2("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/sampling_effort/dat_yr1_effort.csv") #In macbook pro

#---------------------  SKIP  ---------------------------------

# Effort data (revision level)     
  video_yr1 <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/sampling_effort/sampling_effort_yr1/Video_yr1.csv", sep=",")   #In Mac sobremesa EBD

#Stack efforts for plants that had 2 cameras (i.e Olea or Arbutus)
plant_id <- video_yr1$Deployment_ID %>%
  str_remove_all("_") %>%
  str_sub (start =1L, end = 7) %>%
  str_to_lower()

revision_id <- str_remove_all(video_yr1$Revision_ID, "_") %>%
  str_to_lower()

effort_by_plant_yr1 <- data.frame (video_yr1 %>% 
  mutate(ID = str_c(plant_id, revision_id, sep= "_")) %>%
  group_by(ID) %>%
  add_count(ID) %>%     # Add a column with the Count of number of 
  summarise(Deployment_ID = paste(Deployment_ID, collapse = ', '),
            Revision_ID = str_sub(ID, -5), 
            Videos = sum(as.numeric(Videos)),
            Days = sum (Days),
            Days.in.field= sum (Days.in.field),
            n_cam = (n),
            TI = as.logical(sum(Timestamp_Issues, na.rm = TRUE))))

#Merge effort data (some) to new_dat_yr1 (all)
dat_yr1_effort <- left_join(new_dat_yr1, effort_by_plant_yr1, by = "ID", keep = F) %>% 
  mutate(Plant_ID = str_sub(Plant_ID, start = 1L, end = 7)) %>% 
  select(new_path, plant, Plant_ID, Revision_ID, Deployment_ID, File, DateTime, Sp1, Count1, Sp2, Count2, Behaviour, Behaviour2, duration, Days, Videos, Days.in.field, n_cam, Favourite, Obs, TI) %>%
  distinct()  #Remove duplicated rows 

 #write.csv2(dat_yr1_effort, "/Users/Pablo/Desktop/dat_yr1_effort.csv")
```

### 7. Introduce COORDINATES and LOCATION                       - 6,053 rows
```{r}
#In case dont want to run the code below and start from this file 
dat_location <- read.csv2("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/location/data_location_yr1_unfixed_TI.csv", sep = ",")
  
#----------------------------------------------------------
deployments <- read.csv("/Users/Pablo/Documents/GitHub/SUMHAL_WP5_fieldwork/Deployments.csv", sep=";")   #In Mac sobremesa EBD
    
  deployments <- deployments %>%
      select(Deployment_ID, Location, Long, Lat, Start, End, Days, Camera_ID, Timestamp_Issues)

Plant_ID <- deployments$Deployment_ID %>%
  str_remove_all ("_") %>%
  str_sub (start =1L, end = 7) %>%
  str_to_lower()

deplo <- data.frame(cbind (deployments, Plant_ID) %>%
  group_by(Plant_ID) %>%
  summarise(Plant_ID = first(Plant_ID),
            Camera_ID = paste(Camera_ID, collapse = ', '),
            Long = first(Long),
            Lat = first(Lat),
            Location = first(Location)))
   
dat_location <- left_join(dat_yr1_effort, deplo, by = "Plant_ID", keep = FALSE) #Join to deployment information

 #write.csv(dat_location, "/Users/Pablo/Desktop/final_data_yr1_unfixed_TI.csv")
```

### 8. Split entries with two species.                          - 6,090 rows
  We will duplicate the entries where there are two species (SP2 = "non empty") and rename Sp1 with Sp2 name. The rest of the columns will maintain invariable (traceable through Plant_ID/Rev/File). A new column "Coexistence" will be created for searching events with more than one species. 

```{r}
#Select only eating events(just to change name as I already filtered by behavior after introducing duration)
eating_yr1 <- dat_location %>%
  filter(Behaviour == "eating"|Behaviour == "probably eating"| Behaviour == "searching for food")

# Rows with only one species 
rows_wo_sp2 <- eating_yr1 %>%
 filter(is.na(Sp2) | nchar(Sp2) == 0) %>%
  mutate(Coexistence = FALSE)

# Rows with two species (maintaining Sp1)  #Note that there are two species, but not necesarily the sp2 is eating!!
new_rows_for_sp1 <- eating_yr1 %>%
  filter(!is.na(Sp2) & nchar(Sp2) > 0) %>%
  mutate(Coexistence = TRUE)

# Rows with two species - maintaining Sp2 (Duplicate rows based on Sp2 and Behaviour2 conditions) 
new_rows_for_sp2 <- eating_yr1 %>%
  filter(!is.na(Sp2) & nchar(Sp2) > 0 & Behaviour2 %in% c("eating", "probably eating", "searching for food")) %>%
  mutate(Sp1 = Sp2,
         Behaviour = Behaviour2,
         Coexistence = TRUE)

# Combine the original data with the duplicated rows
new_df_yr1 <- bind_rows(rows_wo_sp2 , new_rows_for_sp1, new_rows_for_sp2) %>%
  select(!c (Sp2, Count2, Behaviour2))

```

### 9. Collapse videos to 5 minutes intervals                   - 3,053 rows 
  This code creates a new variable interval that rounds the DateTime variable to the nearest 5-minute interval, and groups the data by both interval and Sp1. The summarize function then collapses the data within each group based on the new interval variable and Sp1 by aggregating the variables with the desired summary functions. Finally, it adds a new variable videos_events that calculates the intensity as the square root of the number of 5-minute intervals per hour (i.e., sqrt(n*12)).

```{r}
#MAKE SURE THAT THIS IS CORRECT and keep on from here
collapsed_data_yr1 <- new_df_yr1 %>%
  group_by(Plant_ID.x, Revision_ID) %>%
  mutate(interval = as.POSIXct(round(as.numeric(as.POSIXct(DateTime)) / (60*5)) * (60*5), origin="1970-01-01")) %>%
  group_by(interval, Sp1) %>%
  summarize(File = paste(File, collapse=', '),
            Plant = first(plant),
            Plant_ID = first(Plant_ID.x),
            Revision_ID = first(Revision_ID),
            DateTime = first(as.POSIXct(DateTime)),
            Species = paste(Sp1, collapse=', '),
            Behaviour = paste(Behaviour, collapse=', '),
            Coexistence = first(Coexistence),
            Deployment_ID = first(Deployment_ID),
            n_cam = first(n_cam),
            Videos = mean(Videos),
            Days = mean(as.numeric(Days)),
            Days.in.field = first(Days.in.field),
            n = max(Count1),
            TI = first(TI),
            duration = sum(as.numeric(duration)),
            long = first(Long),
            lat = first(Lat)) %>%
        mutate(videos_events = sqrt(n*12),  # 5 minute interval, so 12 intervals per hour
         ID = str_c(Plant_ID, Revision_ID, str_sub(File, start = 1L, end = 8), sep = "_")) %>% #To select later on Timestamp Issue fixing
          data.frame()


 #write.csv2(collapsed_data_yr1, file = "/Users/Pablo/Desktop/sumhal_yr1_unfixed_TI.csv")
```

### 10. Fix Timestamp Issues                                    - 2,943 rows
Los TI de las browning no son fáciles de resolver. Los dejaré para más adelante y seguiré con el merging. 
```{r}
sumhal_yr1 <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/sumhal_yr1.csv", sep=";")

#Get Timestamp Issues detected in field notebook.
TI <- sumhal_yr1 %>%
 filter(TI == TRUE) %>%
  data.frame()  #96 TIs

list_TI <- TI$ID 
  sort(list_TI)  #Get the list of TI from the field notebook

# Subset the data frame to get rows with dates before January 1, 2023 (Just to get the order of TI fixing - ordered by date)
df_subset <- subset(sumhal_yr1, DateTime < as.POSIXct("2021-07-15"))  #95 rows

df_subset$ID

#Here the list with TI (before 2020) Note that there could be more TI with only incorrect time. 
list_ID <- df_subset$ID

#Fix dates based in the video ID (generated when collapsing)  - Note that i eliminated Ppin from TI correcting
new_df_yr1_TI_correct  <- sumhal_yr1 %>%
        filter(Plant != "pinus") %>%  #Remove Pinus from the YR1 dataset (not included in datapaper as it´s not fleshy)
        mutate(Dt_correct = DateTime,
               #TI detected in field notebook => TI_list 
              Dt_correct = ifelse (ID == "a10_rev02_IMG_0002"   , "2021-08-05 22:11:00" , Dt_correct),    
              Dt_correct = ifelse (ID == "a10_rev02_IMG_0009"   , "2021-08-10 19:32:00" , Dt_correct),   
              Dt_correct = ifelse (ID == "a12_rev07_01020004"   , "2021-10-22 09:53:00" , Dt_correct),
              Dt_correct = ifelse (ID == "aspa003_rev03_IMG_007", "2021-12-15 10:57:00" , Dt_correct),
              Dt_correct = ifelse (ID == "aspa003_rev03_IMG_008", "2021-12-15 11:46:00" , Dt_correct),
              Dt_correct = ifelse (ID == "aspa003_rev03_IMG_012", "2021-12-15 17:51:00" , Dt_correct),
              Dt_correct = ifelse (ID == "aspa003_rev03_IMG_013", "2021-12-15 18:29:00" , Dt_correct),
              Dt_correct = ifelse (ID == "aspa003_rev03_IMG_014", "2021-12-15 18:37:00" , Dt_correct),
              Dt_correct = ifelse (ID == "aspa003_rev03_IMG_020", "2021-12-16 11:32:00" , Dt_correct),
              Dt_correct = ifelse (ID == "aspa003_rev03_IMG_025", "2021-12-17 08:48:00" , Dt_correct),
              Dt_correct = ifelse (ID == "aspa003_rev03_IMG_027", "2021-12-17 08:54:00" , Dt_correct),
              Dt_correct = ifelse (ID == "aspa003_rev03_IMG_029", "2021-12-17 09:02:00" , Dt_correct),
              Dt_correct = ifelse (ID == "aspa003_rev03_IMG_032", "2021-12-18 15:34:00" , Dt_correct),
              Dt_correct = ifelse (ID == "aspa003_rev03_IMG_033", "2021-12-18 17:23:00" , Dt_correct),
              Dt_correct = ifelse (ID == "aspa003_rev03_IMG_035", "2021-12-18 17:51:00" , Dt_correct),
              Dt_correct = ifelse (ID == "aspa003_rev03_IMG_038", "2021-12-21 12:55:00" , Dt_correct),
              Dt_correct = ifelse (ID == "aspa003_rev03_IMG_045", "2021-12-21 10:04:00" , Dt_correct),
              Dt_correct = ifelse (ID == "joxy001_rev01_IMG_005", "2021-12-16 19:11:00" , Dt_correct),
              Dt_correct = ifelse (ID == "joxy001_rev01_IMG_016", "2021-12-17 22:42:00" , Dt_correct),
              Dt_correct = ifelse (ID == "joxy001_rev01_IMG_017", "2021-12-16 22:52:00" , Dt_correct),
              Dt_correct = ifelse (ID == "joxy001_rev01_IMG_026", "2022-01-01 17:50:00" , Dt_correct),
              Dt_correct = ifelse (ID == "joxy001_rev01_IMG_029", "2022-01-01 17:58:00" , Dt_correct),
              Dt_correct = ifelse (ID == "joxy001_rev01_IMG_047", "2022-01-12 01:36:00" , Dt_correct),
              Dt_correct = ifelse (ID == "pbou002_rev01_0127037", "2021-09-28 08:02:00" , Dt_correct),
              Dt_correct = ifelse (ID == "pbou002_rev03_0224008", "2021-10-26 23:21:00" , Dt_correct),
              Dt_correct = ifelse (ID == "pbou002_rev03_0224018", "2021-10-27 06:23:00" , Dt_correct),
              Dt_correct = ifelse (ID == "pbou002_rev03_0224023", "2021-10-27 06:32:00" , Dt_correct),
              Dt_correct = ifelse (ID == "pbou002_rev03_0226075", "2021-10-28 22:40:00" , Dt_correct),
              Dt_correct = ifelse (ID == "pbou002_rev03_0226094", "2021-10-29 11:10:00" , Dt_correct),
              Dt_correct = ifelse (ID == "pbou002_rev04_0311051", "2021-11-09 16:05:00" , Dt_correct),
              Dt_correct = ifelse (ID == "pbou002_rev04_0311058", "2021-11-09 18:25:00" , Dt_correct),
              Dt_correct = ifelse (ID == "pbou003_rev04_1027009", "2021-11-10 14:55:00" , Dt_correct),
              Dt_correct = ifelse (ID == "pbou003_rev04_1031048", "2021-11-14 09:17:00" , Dt_correct),
              Dt_correct = ifelse (ID == "pbou003_rev04_1103076", "2021-11-17 18:55:00" , Dt_correct),
              Dt_correct = ifelse (ID == "pbou003_rev04_1103083", "2021-11-17 19:04:00" , Dt_correct),
              Dt_correct = ifelse (ID == "pbou004_rev02_0101002", "2021-10-14 18:59:00" , Dt_correct),
              Dt_correct = ifelse (ID == "pbou004_rev02_0112024", "2021-10-25 18:07:00" , Dt_correct),
              Dt_correct = ifelse (ID == "rper001_rev02_IMG_076", "2021-11-02 11:38:00" , Dt_correct),
              Dt_correct = ifelse (ID == "rulm005_rev01_IMG_056", "2021-08-27 03:29:00" , Dt_correct),
              Dt_correct = ifelse (ID == "rulm005_rev02_IMG_063", "2021-09-08 07:03:00" , Dt_correct),
              Dt_correct = ifelse (ID == "rulm005_rev02_IMG_067", "2021-09-08 07:32:00" , Dt_correct),
              Dt_correct = ifelse (ID == "rulm005_rev02_IMG_071", "2021-09-08 07:49:00" , Dt_correct),
              Dt_correct = ifelse (ID == "rulm005_rev02_IMG_093", "2021-09-08 11:32:00" , Dt_correct),
              Dt_correct = ifelse (ID == "rulm005_rev02_IMG_085", "2021-09-08 10:28:00" , Dt_correct),
              Dt_correct = ifelse (ID == "rulm005_rev02_IMG_086", "2021-09-08 10:31:00" , Dt_correct),
              Dt_correct = ifelse (ID == "rulm005_rev02_IMG_095", "2021-09-08 11:34:00" , Dt_correct),
              Dt_correct = ifelse (ID == "rulm005_rev02_IMG_025", "2021-09-06 19:57:00" , Dt_correct),
              Dt_correct = ifelse (ID == "rulm005_rev02_IMG_029", "2021-09-06 23:24:00" , Dt_correct),
              Dt_correct = ifelse (ID == "rulm005_rev04_IMG_015", "2021-10-04 05:40:00" , Dt_correct),
              Dt_correct = ifelse (ID == "rulm005_rev04_IMG_082", "2021-10-16 06:37:00" , Dt_correct),
              Dt_correct = ifelse (ID == "rulm016_rev01_IMG_047", "2021-08-23 18:56:00" , Dt_correct),
              Dt_correct = ifelse (ID == "rulm016_rev01_IMG_048", "2021-08-23 18:57:00" , Dt_correct),
              Dt_correct = ifelse (ID == "rulm016_rev01_IMG_065", "2021-08-25 11:13:00" , Dt_correct),
              Dt_correct = ifelse (ID == "rulm016_rev01_IMG_053", "2021-08-24 11:04:00" , Dt_correct),
              Dt_correct = ifelse (ID == "rulm016_rev02_IMG_029", "2021-09-10 23:43:00" , Dt_correct),
              Dt_correct = ifelse (ID == "rulm016_rev02_IMG_020", "2021-09-09 04:53:00" , Dt_correct),
              Dt_correct = ifelse (ID == "rulm016_rev02_IMG_032", "2021-09-11 03:53:00" , Dt_correct),
              Dt_correct = ifelse (ID == "rulm016_rev02_IMG_001", "2021-09-03 21:22:00" , Dt_correct),
              Dt_correct = ifelse (ID == "rulm016_rev03_IMG_072", "2021-09-17 10:23:00" , Dt_correct),
              Dt_correct = ifelse (ID == "rulm016_rev03_IMG_073", "2021-09-17 10:24:00" , Dt_correct),
              Dt_correct = ifelse (ID == "rulm016_rev03_IMG_011", "2021-09-19 02:20:00" , Dt_correct),
              Dt_correct = ifelse (ID == "rulm016_rev04_IMG_034", "2021-10-11 09:34:00" , Dt_correct),
              Dt_correct = ifelse (ID == "rulm016_rev04_IMG_033", "2021-10-10 23:32:00" , Dt_correct),
              Dt_correct = ifelse (ID == "rulm021_rev03_IMG_012", "2021-09-15 13:26:00" , Dt_correct),  #TI  from data previous to   "2021-07-15" 
               Dt_correct = ifelse ( ID == "pbou004_rev02_01010002", "2021-10-14 19:29:00" , Dt_correct),
               Dt_correct = ifelse ( ID == "a12_rev07_01020004"    , "2021-10-14 23:34:00" , Dt_correct),
               Dt_correct = ifelse ( ID == "pbou004_rev02_01120724", "2021-10-25 18:37:00" , Dt_correct),
               Dt_correct = ifelse ( ID == "pbou004_rev03_01130048", "2021-10-27 13:39:00" , Dt_correct),
               Dt_correct = ifelse ( ID == "pbou002_rev01_01270037", "2021-09-29 08:04:00" , Dt_correct),
               Dt_correct = ifelse ( ID == "pbou002_rev03_02240008", "2021-10-27 02:58:00" , Dt_correct),
               Dt_correct = ifelse ( ID == "pbou002_rev03_02240018", "2021-10-27 10:00:00" , Dt_correct),
               Dt_correct = ifelse ( ID == "pbou002_rev03_02240023", "2021-10-27 10:09:00" , Dt_correct),
               Dt_correct = ifelse ( ID == "pbou002_rev03_02260075", "2021-10-29 02:17:00" , Dt_correct),
               Dt_correct = ifelse ( ID == "pbou002_rev03_02260094", "2021-10-29 14:47:00" , Dt_correct),
               Dt_correct = ifelse ( ID == "pbou002_rev03_03060714", "2021-11-06 15:11:00" , Dt_correct),
               Dt_correct = ifelse ( ID == "pbou002_rev03_03090254", "2021-11-09 00:25:00" , Dt_correct),
               Dt_correct = ifelse ( ID == "pbou002_rev03_03090256", "2021-11-09 00:53:00" , Dt_correct),
               Dt_correct = ifelse ( ID == "pbou002_rev04_03110151", "2021-11-10 16:47:00" , Dt_correct),
               Dt_correct = ifelse ( ID == "pbou002_rev04_03110158", "2021-11-10 17:21:00" , Dt_correct),
               Dt_correct = ifelse ( ID == "aspa003_rev03_IMG_0007", "2021-12-15 12:35:00" , Dt_correct),
               Dt_correct = ifelse ( ID == "aspa003_rev03_IMG_0008", "2021-12-15 14:26:00" , Dt_correct),
               Dt_correct = ifelse ( ID == "aspa003_rev03_IMG_0012", "2021-12-15 19:31:00" , Dt_correct),
               Dt_correct = ifelse ( ID == "aspa003_rev03_IMG_0013", "2021-12-15 20:09:00" , Dt_correct),
               Dt_correct = ifelse ( ID == "aspa003_rev03_IMG_0014", "2021-12-15 20:17:00" , Dt_correct),
               Dt_correct = ifelse ( ID == "aspa003_rev03_IMG_0020", "2021-12-16 14:12:00" , Dt_correct),
               Dt_correct = ifelse ( ID == "aspa003_rev03_IMG_0025", "2021-12-17 11:28:00", Dt_correct) ,
               Dt_correct = ifelse ( ID == "aspa003_rev03_IMG_0027", "2021-12-17 15:34:00", Dt_correct),
               Dt_correct = ifelse ( ID == "aspa003_rev03_IMG_0029", "2021-12-17 16:42:00", Dt_correct),
               Dt_correct = ifelse ( ID == "aspa003_rev03_IMG_0032", "2021-12-18 18:13:00", Dt_correct),
               Dt_correct = ifelse ( ID == "aspa003_rev03_IMG_0033", "2021-12-18 20:03:00", Dt_correct),
               Dt_correct = ifelse ( ID == "aspa003_rev03_IMG_0035", "2021-12-18 20:31:00", Dt_correct),
               Dt_correct = ifelse ( ID == "aspa003_rev03_IMG_0038", "2021-12-19 15:35:00", Dt_correct),
               Dt_correct = ifelse ( ID == "aspa003_rev03_IMG_0045", "2021-12-21 12:44:00", Dt_correct),
               Dt_correct = ifelse ( ID == "a10_rev02_IMG_0009"  ,   "2021-08-09 23:52:00" , Dt_correct),
               Dt_correct = ifelse ( ID == "joxy001_rev01_IMG_0005", "2021-12-16 20:27:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm021_rev02_IMG_0002", "2021-09-30 06:55:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm016_rev01_IMG_0047", "2021-08-23 22:25:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm016_rev01_IMG_0048", "2021-08-23 22:26:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm016_rev01_IMG_0065", "2021-08-25 14:42:00", Dt_correct),
               Dt_correct = ifelse ( ID == "joxy001_rev01_IMG_0016", "2020-01-08 10:39:00", Dt_correct),
               Dt_correct = ifelse ( ID == "joxy001_rev01_IMG_0017", "2020-01-08 10:39:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm005_rev01_IMG_0056", "2021-08-27 03:13:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm021_rev03_IMG_0091", "2021-09-29 08:39:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm016_rev01_IMG_0253", "2021-09-03 07:35:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm021_rev03_IMG_0112", "2021-09-30 12:10:00", Dt_correct),
               Dt_correct = ifelse ( ID == "oeur001_rev02_IMG_0225", "2021-11-11 16:11:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rper001_rev02_IMG_0076", "2021-10-31 11:38:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm005_rev02_IMG_0063", "2021-09-08 07:03:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm005_rev02_IMG_0067", "2021-09-08 07:32:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm005_rev02_IMG_0071", "2021-09-08 07:49:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm005_rev02_IMG_0093", "2021-09-08 11:32:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm016_rev02_IMG_0229", "2021-09-09 03:27:00", Dt_correct),
               Dt_correct = ifelse ( ID == "oeur001_rev02_IMG_0334", "2021-11-16 09:52:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm005_rev02_IMG_0185", "2021-09-10 02:15:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm005_rev02_IMG_0186", "2021-09-10 06:38:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm005_rev02_IMG_0195", "2021-09-10 10:10:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm016_rev02_IMG_0287", "2021-09-10 21:26:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm016_rev02_IMG_0292", "2021-09-10 21:31:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm005_rev02_IMG_0225", "2021-09-10 21:00:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm005_rev02_IMG_0229", "2021-09-10 21:14:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm016_rev02_IMG_0320", "2021-09-11 01:37:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm016_rev02_IMG_0332", "2021-09-11 02:18:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm016_rev02_IMG_0364", "2021-09-11 19:36:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm016_rev02_IMG_0370", "2021-09-11 19:46:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm016_rev02_IMG_0401", "2021-09-12 04:24:00", Dt_correct),
               Dt_correct = ifelse ( ID == "joxy001_rev01_IMG_0047", "2020-01-27 13:33:00", Dt_correct),
               Dt_correct = ifelse ( ID == "joxy001_rev02_IMG_0004", "2022-01-13 16:33:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm016_rev03_IMG_0072", "2021-09-17 08:17:00", Dt_correct),
               Dt_correct = ifelse ( ID == "joxy001_rev02_IMG_0008", "2022-01-13 16:33:00", Dt_correct),
               Dt_correct = ifelse ( ID == "joxy001_rev02_IMG_0012", "2022-01-14 17:54:00", Dt_correct),
               Dt_correct = ifelse ( ID == "joxy001_rev02_IMG_0020", "2022-01-14 18:00:00", Dt_correct),
               Dt_correct = ifelse ( ID == "joxy001_rev02_IMG_0026", "2022-01-14 18:03:00", Dt_correct),
               Dt_correct = ifelse ( ID == "joxy001_rev02_IMG_0030", "2022-01-14 18:08:00", Dt_correct),
               Dt_correct = ifelse ( ID == "oeur001_rev03_IMG_0115", "2021-11-27 10:36:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm016_rev03_IMG_0173", "2021-09-23 00:20:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm016_rev03_IMG_0211", "2021-09-25 21:42:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm005_rev04_IMG_0015", "2021-10-01 06:30:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm016_rev04_IMG_0034", "2021-10-02 14:38:00", Dt_correct),
               Dt_correct = ifelse ( ID == "joxy001_rev02_IMG_0048", "2022-01-30 23:10:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm016_rev04_IMG_0133", "2021-10-04 18:14:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm016_rev04_IMG_0243", "2021-10-07 08:46:00", Dt_correct),
               Dt_correct = ifelse ( ID == "rulm005_rev04_IMG_0082", "2021-10-13 05:31:00", Dt_correct),
               Dt_correct = ifelse ( ID == "oeur001_rev04_IMG_0035", "2021-12-23 16:23:00", Dt_correct),
               Dt_correct = ifelse ( ID == "oeur001_rev04_IMG_0042", "2021-12-23 16:45:00", Dt_correct))

write.csv2(new_df_wo_TI, file = "/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/sumhal_yr1.csv", sep = ",")
```


## 2.2 SUMHAL YR2
### 1. Load data                                      - 168,894 rows
   Load results for all species from visualization in Timelapse and add a column with the plant species
```{r}
aune <- read.csv2("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/Timelapse results Yr2/Arbutus_yr2_revised_c01.csv", sep=",") %>%
  select(File, RelativePath, DateTime, Sp1, Count1, Behaviour, Sex, Sp2, Count2, Behaviour2, Sp3, Juvenile, Favourite, Obs) %>%
  mutate(plant = "aune")

aspa <- read.csv2("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/Timelapse results Yr2/Asparagus_yr2_revised_c02.csv", sep=",")%>%
  select(File, RelativePath, DateTime, Sp1, Count1, Behaviour, Sex, Sp2, Count2, Behaviour2, Sp3, Juvenile, Favourite, Obs) %>%
  mutate(plant = "aspa")

mcom <- read.csv2("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/Timelapse results Yr2/Myrtus_yr2_revised_c01.csv", sep=",")%>%
  select(File, RelativePath, DateTime, Sp1, Count1, Behaviour, Sex, Sp2, Count2, Behaviour2, Sp3, Juvenile, Favourite, Obs) %>%
  mutate(plant = "mcom")

oeur <- read.csv2("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/Timelapse results Yr2/Olea_yr2_revised_c01.csv", sep=",")%>%
  select(File, RelativePath, DateTime, Sp1, Count1, Behaviour, Sex, Sp2, Count2, Behaviour2, Sp3, Juvenile, Favourite, Obs) %>%
  mutate(plant = "oeur")

olan <- read.csv2("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/Timelapse results Yr2/Osyris_yr2_revised_c01.csv", sep=",")%>%
  select(File, RelativePath, DateTime, Sp1, Count1, Behaviour, Sex, Sp2, Count2, Behaviour2, Sp3, Juvenile, Favourite, Obs) %>%
  mutate(plant = "olan")

rper <- read.csv2("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/Timelapse results Yr2/Rubia_yr2_revised_c0.1.csv", sep=",")%>%
  select(File, RelativePath, DateTime, Sp1, Count1, Behaviour, Sex, Sp2, Count2, Behaviour2, Sp3, Juvenile, Favourite, Obs) %>%
  mutate(plant = "rper")

rulm <- read.csv2("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/Timelapse results Yr2/Rubus_yr2_revised_c01.csv", sep=";")%>%  
  select(File, RelativePath, DateTime, Sp1, Count1, Behaviour, Sex, Sp2, Count2, Behaviour2, Sp3, Juvenile, Favourite, Obs) %>%
  mutate(RelativePath = str_c("SUMHAL_YR2", RelativePath, sep = "\\")) %>% #Need to change Relative Path for consistency with other plant revisions 
  mutate(plant = "rulm")

sasp <- read.csv2("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/Timelapse results Yr2/Smilax_yr2_revised_c01.csv", sep=",")%>%
  select(File, RelativePath, DateTime, Sp1, Count1, Behaviour, Sex, Sp2, Count2, Behaviour2, Sp3, Juvenile, Favourite, Obs) %>%
  mutate(DateTime = as.POSIXct(DateTime, format="%Y-%m-%d %H:%M:%S")) %>% #Need to change Date format for consistency (from using a different -old- Timelapse template)
  mutate(plant = "sasp")

#Joxy will be run in a parallel script because files are in a different Hard drive and I realised it when the code was already written. It is easyer this way and join the resulting collapsed data later in the phototrapping merging script. 
joxy <- read.csv2("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/Timelapse results Yr2/joxy_yr2_revised_c02.csv", sep=",") %>%  
  select(File, RelativePath, DateTime, Sp1, Count1, Behaviour, Sex, Sp2, Count2, Behaviour2, Sp3, Juvenile, Favourite, Obs) %>%
  mutate(plant = "joxy")

data_yr2 <- rbind(aune, aspa, mcom, oeur, olan, rper, rulm, sasp)
```

### 2. CORRECT INCONSISTENCIES                        - 168,894 rows
  Change incorrect or inconsistent species names (in column Sp1 and Sp2) 
```{r}
#Check for inconsistencies in species names  
sort(unique(data_yr2$Sp1))

#Check strange names in videos (change "sp name" and visualize the video)
data_yr2 %>%
  filter(Sp1 == "spo")

#Change mistaken names in Sp1
data_yr2$Sp1 [data_yr2$Sp1 == "spo" ] <- "apode-mus" 
data_yr2$Sp1 [data_yr2$Sp1 == "turdus oh" ] <- "turdus philomelos"
data_yr2$Sp1 [data_yr2$Sp1 == "cep" ] <- "cervus elaphus"
data_yr2$Sp1 [data_yr2$Sp1 == "se" ] <- "empty" 
data_yr2$Sp1 [data_yr2$Sp1 == "ce" ] <- "cervus elaphus"
data_yr2$Sp1 [data_yr2$Sp1 == "fr" ] <- "fringilla coelebs"
data_yr2$Sp1 [data_yr2$Sp1 == "orictolagus cunniculus" ] <- "oryctolagus cuniculus"
data_yr2$Sp1 [data_yr2$Sp1 == "orictolagus cuniculus" ] <- "oryctolagus cuniculus"
data_yr2$Sp1 [data_yr2$Sp1 == "cervus elacephus" ] <- "cervus elaphus"
data_yr2$Sp1 [data_yr2$Sp1 == "cervues elaphus" ] <- "cervus elaphus"
data_yr2$Sp1 [data_yr2$Sp1 == "hippolais poliglota" ] <- "hippolais poliglotta" 
data_yr2$Sp1 [data_yr2$Sp1 == "Saxicola rubicola" ] <- "saxicola rubicola" 
data_yr2$Sp1 [data_yr2$Sp1 == "eç" ] <- "empty" 
data_yr2$Sp1 [data_yr2$Sp1 == "saxicola torquata" ] <- "saxicola rubicola" 
data_yr2$Sp1 [data_yr2$Sp1 == "parus caeruleus" ] <- "cyanistes caeruleus" 
data_yr2$Sp1 [data_yr2$Sp1 == "Acrocephalus" ] <- "acrocephalus" 
data_yr2$Sp1 [data_yr2$Sp1 == "scom" ] <- "sylvia communis" 
data_yr2$Sp1 [data_yr2$Sp1 == "smel" ] <- "sylvia melanocephala" 
data_yr2$Sp1 [data_yr2$Sp1 == "smle" ] <- "sylvia melanocephala" 
data_yr2$Sp1 [data_yr2$Sp1 == "ggen" ] <- "genetta genetta" 
data_yr2$Sp1 [data_yr2$Sp1 == "musicapa striata" ] <- "muscicapa striata" 
data_yr2$Sp1 [data_yr2$Sp1 == "troglodites" ] <- "troglodytes troglodytes" 
data_yr2$Sp1 [data_yr2$Sp1 == "troglodites troglodites" ] <- "troglodytes troglodytes" 
data_yr2$Sp1 [data_yr2$Sp1 == "thacus rubecula" ] <- "erithacus rubecula"
data_yr2$Sp1 [data_yr2$Sp1 == "emptye" ] <- "empty"
data_yr2$Sp1 [data_yr2$Sp1 == "satr" ] <- "sylvia atricapilla"
data_yr2$Sp1 [data_yr2$Sp1 == "hyppolais poliglota" ] <- "hippolais poliglotta"
data_yr2$Sp1 [data_yr2$Sp1 == "sylvia cosemmunis" ] <- "sylvia communis"
data_yr2$Sp1 [data_yr2$Sp1 == "phyloscopus" ] <- "phylloscopus"
data_yr2$Sp1 [data_yr2$Sp1 == "philoscopus" ] <- "phylloscopus"
data_yr2$Sp1 [data_yr2$Sp1 == "sylvia udata" ] <- "sylvia undata"
data_yr2$Sp1 [data_yr2$Sp1 == "grashoper" ] <- "grasshopper"
data_yr2$Sp1 [data_yr2$Sp1 == "grashopper" ] <- "grasshopper"
data_yr2$Sp1 [data_yr2$Sp1 == "rat" ] <- "rattus"
data_yr2$Sp1 [data_yr2$Sp1 == "dog" ] <- "canis familiaris"
data_yr2$Sp1 [data_yr2$Sp1 == "bos tausus" ] <- "bos taurus"
data_yr2$Sp1 [data_yr2$Sp1 == "Jinx torquilla" ] <- "jinx torquilla"

#Change mistaken names in Sp2
data_yr2$Sp2 [data_yr2$Sp2 == "barn swallow" ] <- "hirundo rustica"
data_yr2$Sp2 [data_yr2$Sp2 == "ce" ] <- "cervus elaphus"
data_yr2$Sp2 [data_yr2$Sp2 == "eritbacus rubecula" ] <- "erithacus rubecula"
data_yr2$Sp2 [data_yr2$Sp2 == "Hirundo daurica" ] <- "hirundo daurica"
data_yr2$Sp2 [data_yr2$Sp2 == "parus caerulius" ] <- "cyanistes caeruleus"
data_yr2$Sp2 [data_yr2$Sp2 == "satr" ] <- "sylvia atricapilla"
data_yr2$Sp2 [data_yr2$Sp2 == "scom" ] <- "sylvia communis"
data_yr2$Sp2 [data_yr2$Sp2 == "smel" ] <- "sylvia melanocephala"
data_yr2$Sp2 [data_yr2$Sp2 == "Sylvia melanocephala" ] <- "sylvia melanocephala"
data_yr2$Sp2 [data_yr2$Sp2 == "turdus me" ] <- "turdus merula"
```

### 3. FILTER non animals                             - 14,199 rows rows
  select only data with interactions (based on eating behaviors) & remove non animal/empty rows
```{r}
eating_yr2 <- data_yr2 %>%
  filter(Behaviour == "eating" | Behaviour == "probably eating" | Behaviour == "searching for food")

#Only select videos with animals & eliminate car, person.
dat_yr2 <- data_yr2 %>%
  filter(Sp1 != "",
         Sp1 != "empty",
         Sp1 != "car",
         Sp1 != "person")

         
#Change Count to the number of individuals  
dat_yr2$Count1 [dat_yr2$Count1 == 0 ] <- 1
dat_yr2$Count2 <- ifelse(dat_yr2$Sp2 != "", 1, 0)

```

### 4. Include VIDEO DURATION                         - 27,539 rows
```{r}
#Just in case dont want to run the time demanding code.
dat_yr2_duration <- read.csv2 ("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/duration/dat_yr2_duration.csv", sep=",")

#----------------------  SKIP  ------------------------------------------------- 
# Get a list of all files in GRAID 
dir_yr2 <- list.dirs ("/Volumes/G-RAID-3/SUMHAL_YR2") #Lista de todos los directorios en el Rack
fil_yr2 <- list.files(dir_yr2, full.names = T, recursive = TRUE, include.dirs = FALSE) #Lista de archivos con su path (sin directorios)
    #write.csv(fil_yr2, "/Users/PV/Desktop/video_file_list_yr2.csv", sep = ",")

# Get a list of all files in La Cie 
dir_yr2_2 <- list.dirs ("/Volumes/LaCie/SUMHAL_YR2") #Lista de todos los directorios en el Rack
fil_yr2_2 <- list.files(dir_yr2_2, full.names = T, recursive = TRUE, include.dirs = FALSE)  #Lista de archivos con sus directorios
    #write.csv(fil_yr2_2, "/Users/PV/Desktop/video_file_list_yr2_joxy.csv", sep = ",")

# Get a list of selected files   
files_yr2_dat <- str_c(dat_yr2$RelativePath, dat_yr2$File, sep ="/") %>%  #Lista de videos eating
  str_replace_all("\\\\", "/") #change syntax for  
files_yr2_dat_path <- file.path ("/Volumes/G-RAID-3", files_yr2_dat) #Lista de videos eating con su path

#Path for Joxy must be LaCie while path for the rest should be G-RAID-3
joxy_files_dat <- dat_yr2 %>%
  filter (plant == "joxy") %>%
  mutate(FILE = str_c(dat_yr2$RelativePath, dat_yr2$File, sep ="/")) %>%  #Lista de videos eating
  mutate(FILE = str_replace_all("\\\\", "/")) #change syntax for  

    joxy_files_dat_path <- file.path ("/Volumes/LaCie", joxy_files_dat)
  
unique(dat_yr2$plant_yr2)
str(dat_yr2)

# Compare eating list to the entire file list (all files in GRAID) to detect missmatches  
fil_lost <- setdiff(files_yr2_dat_path, fil_yr2) #What is in my_list_2 that is not in my_list_1?
length(fil_lost)

# Extract video duration from eating list (have it already run in Results folder)
info <- lapply(files_yr2_dat_path, av_media_info)  #apply info extraction function to video list (Time demanding)
duration <- sapply(info, function(x){as.numeric(x[1])}) #Extract only video duration (position 1)

 #write.csv(info, "/Users/PV/Desktop/video_info_yr2.csv", row.names = FALSE)
#info <- read.csv2("/Users/PV/Documents/GitHub/Animal-detection-cameratrap/Results/video_info_yr2.csv", sep = ",")
str(info)

# Join duration to database  
dat_yr2_duration <- cbind(dat_yr2, duration)

#Change Count to the number of individuals  
dat_yr2_duration$Count1 [dat_yr2_duration$Count1 == 0 ] <- 1
dat_yr2_duration$Count2 <- ifelse(dat_yr2_duration$Sp2 != "", 1, 0)

    #write.csv(dat_yr2_duration,"/Users/PV/Desktop/dat_yr2_duration.csv")
#-----------------------------------------------------------------------

```

### 5. Create NEW COLUMNS                             - 27,539 rows
  Plant_ID, Revision id and new DF
```{r}
## CLEAN THE DATA (Video level == Data non collapsed to 5 min)
#Split path in different columns  
clean <- data.frame(str_split_fixed(dat_yr2_duration$RelativePath, pattern ="\\\\", 4))
colnames(clean) <- c("folder", "Plant_sp", "Rev", "Plant_ID")

Plant_ID <- data.frame(clean$Plant_ID) %>%
  rename(Plant_ID = clean.Plant_ID)

Rev <- data.frame (str_sub(clean$Rev, start = 6L, end = 20)) %>%
  rename(Rev = str_sub.clean.Rev..start...6L..end...20.)


new_dat_yr2 <- dat_yr2_duration %>%
  select(File, RelativePath, DateTime, Sp1, Behaviour, Count1, Sp2, Behaviour2, Count2, Sp3, plant, duration, Favourite, Obs) %>%
  cbind(Plant_ID, Rev) %>%
  mutate(ID = str_to_lower(str_c (str_sub(Plant_ID, start = 1L, end = 7), str_sub(Rev, start = 1L, end = 5), sep="_")))

```

### 6. Introduce SAMPLING EFFORT                      - 14,198 rows
  There is a code to automatize the extraction of video file information regarding to Sampling effort at deployment and upper level. The script is called videofile_batch_data_extraction and creates the video_yr2.csv document.  
```{r}
#In case you want to skip the code 
dat_yr2_effort <- read.csv2("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/sampling_effort/dat_yr2_effort.csv") #In macbook pro

#----------------------------- SKIP ----------------
# Effort data (revision level)
video_yr2 <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/sampling_effort/sampling_effort_yr2/video_yr2.csv", sep=";")     #In MAcbook pro   

#Stack efforts for plants that had 2 cameras (i.e Olea or Arbutus)
plant_id <- video_yr2$deployment %>%
  str_remove_all("_") %>%
  str_sub (start =1L, end = 7) %>%
  str_to_lower()

revision_id <- str_remove_all(video_yr2$rev, "_") %>%
  str_to_lower()

effort_by_plant <- data.frame (video_yr2 %>% 
  mutate(ID = str_c(plant_id, revision_id, sep= "_"))%>%
  group_by(ID) %>%
  add_count(ID) %>%     # Add a column with the Count of number of??
  summarise(Deployment_ID = paste(deployment, collapse = ', '),
            Revision_ID = first(rev), 
            Videos = sum(num_files),
            Days = sum (num_days),
            Days.in.field= sum (Functioning.days),
            n_cam = (n),
            TI = as.logical(sum(Timestamp_Issues, na.rm = TRUE))))

#Merge effort data (some) to new_dat (all)
dat_yr2_effort <- left_join(new_dat_yr2, effort_by_plant, by = "ID", keep = F) %>%
  mutate(Plant_ID = str_sub(Plant_ID, start = 1L, end = 7)) %>% 
  select(plant, Plant_ID, Revision_ID, Deployment_ID, File, DateTime, Sp1, Count1, Sp2, Count2, Behaviour, Behaviour2, duration, Days, Videos, Days.in.field, n_cam, Favourite, Obs, TI) %>%
  distinct() %>%  #Remove duplicated rows 
  filter(Sp1 != "empty")

summary(dat_yr2_effort$TI) #Number of Timestamp Issues

#write.csv2(dat_yr2_effort, "/Users/PV/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/dat_yr2_effort.csv")

sort(unique(dat_yr2_effort$Plant_ID))

```

### 7. Introduce COORDINATES and LOCATION             - 14,198 rows
```{r}
  deployments <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/sampling_effort/Deployments_yr2.csv", sep=";") #In Mac sobremesa

deployments <- deployments %>%
    mutate(Camera_ID = str_sub (Deployment_ID, start = 9, end = 12)) %>%
    select(Deployment_ID, Long, Lat, Start, End, Days, Camera_ID, Timestamp_Issues)

Plant_ID <- deployments$Deployment_ID %>%
  str_remove_all ("_") %>%
  str_sub (start =1L, end = 7) %>%
  str_to_title()

deplo <- data.frame(cbind (deployments, Plant_ID) %>%
  group_by(Plant_ID) %>%
  summarise(Plant_ID = first(Plant_ID),
            Camera_ID = paste(Camera_ID, collapse = ', '),
            Long = first(Long),
            Lat = first(Lat)))
   

dat_yr2_location <- left_join(dat_yr2_effort, deplo, by = "Plant_ID", keep = TRUE) 

#write.csv2(dat_yr2_location , "/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/final_data_yr2_unfixed_TI.csv")

dat_yr2_location <- read.csv2("/Users/PV/Documents/GitHub/Animal-detection-cameratrap/Results/final_data_yr2_unfixed_TI.csv")

```

### 8. FILTER eating behaviours & SELECT columns      - 8,998 rows
```{r}
eating_yr2 <- dat_yr2_location %>%
  filter(Behaviour == "eating" |Behaviour == "probably eating" |Behaviour == "searching for food") %>%
  select(plant, Plant_ID.x, Revision_ID, Deployment_ID, File, DateTime, Sp1, Behaviour, Count1, Sp2, Count2, Behaviour2, duration, Days, Days.in.field, Videos, n_cam, Favourite, Obs, TI, Long, Lat) %>%
  rename(Plant_ID = Plant_ID.x, Revision = Revision_ID, Deployment = Deployment_ID, Duration = duration, Days_in_field = Days.in.field)%>%
  mutate(Duration = as.numeric(Duration))

str(eating_yr2)
```

### 9. Split entries with two species                 - 9,102 rows
  We will duplicate the entries where there are two species (SP2 = "non empty") and rename Sp1 with Sp2 name. The rest of the columns will maintain invariable (traceable through Plant_ID/Rev/File). A new column "Coexistence" will be created for searching events with more than one species. 

```{r}
# Rows with only one species 
rows_wo_sp2 <- eating_yr2 %>%
 filter(is.na(Sp2) | nchar(Sp2) == 0) %>%
  mutate(Coexistence = FALSE)

# Rows with two species (maintaining Sp1)  #Note that there are two species, but not necesarily the sp2 is eating!!
new_rows_for_sp1 <- eating_yr2 %>%
  filter(!is.na(Sp2) & nchar(Sp2) > 0) %>%
  mutate(Coexistence = TRUE)

# Rows with two species - maintaining Sp2 (Duplicate rows based on Sp2 and Behaviour2 conditions) 
new_rows_for_sp2 <- eating_yr2 %>%
  filter(!is.na(Sp2) & nchar(Sp2) > 0 & Behaviour2 %in% c("eating", "probably eating", "searching for food")) %>%
  mutate(Sp1 = Sp2,
         Behaviour = Behaviour2,
         Coexistence = TRUE)

# Combine the original data with the duplicated rows
new_df <- bind_rows(rows_wo_sp2 , new_rows_for_sp1, new_rows_for_sp2) %>%
  select(!c (Sp2, Count2, Behaviour2)) 

```

### 10. FIX SMILAX DATE Format                        - 9,102 rows
  First need to fix the date format from Smilax that was revised with an old version of TimeLapse Template. 
```{r}
#Change Smilax date format
sasp_dates <- new_df %>%
  filter(plant == "sasp") %>%
  mutate(DateTime = as.POSIXct(DateTime, format= "%d/%m/%Y %H:%M"))

#Maintain the rest of the Species date format and 
rest_dates <- new_df %>%
  filter(plant != "sasp") 

#Bind both df
fixed_dates <- rbind(sasp_dates, rest_dates) 
    
```

### 11. Collapse videos to 5 minutes intervals        - 5,304 rows
  This code creates a new variable interval that rounds the DateTime variable to the nearest 5-minute interval, and groups the data by both interval and Sp1. The summarize function then collapses the data within each group based on the new interval variable and Sp1 by aggregating the variables with the desired summary functions. Finally, it adds a new variable videos_events that calculates the intensity as the square root of the number of 5-minute intervals per hour (i.e., sqrt(n*12)).

```{r}
#check but it looks fine to me today.
collapsed_data <- fixed_dates %>%
  group_by(Plant_ID, Revision) %>%
  mutate(interval = as.POSIXct(round(as.numeric(as.POSIXct(DateTime)) / (60*5)) * (60*5), origin="1970-01-01")) %>%
  group_by(interval, Sp1) %>%
  summarize(File = paste(File, collapse=', '),
            Plant = first(plant),
            Plant_ID = first(Plant_ID),
            Revision = first(Revision),
            DateTime = first(DateTime),
            Species = paste(Sp1, collapse=', '),
            Behaviour = paste(Behaviour, collapse=', '),
            Coexistence = first(Coexistence),
            Deployment_ID = first(Deployment),
            n_cam = first(n_cam),
            Videos = mean(Videos),
            Days = mean(Days),
            Days.in.field = first(Days_in_field),
            n = sum(Count1),
            Duration = sum(Duration),
            long = first(Long),
            lat = first(Lat)) %>%
  mutate(videos_events = sqrt(n*12),  # 5 minute interval, so 12 intervals per hour
         ID = str_c(Plant_ID, Revision, str_sub(File, start = 1L, end = 8), sep = "_")) #To select later on Timestamp Issue fixing
  
```

### 12. Fix Timestamp Issues                          - 5,304 rows
```{r}
# Subset the data frame to get rows with dates before January 1, 2023 (Just to get the order of TI fixing - ordered by date)
df_subset <- subset(collapsed_data, DateTime < as.POSIXct("2022-01-01"))
  
#Here the list with TI (before 2020) Note that there could be mor TI with only incorrect time. 
list_ID <- df_subset$ID

#Fix dates based in the video ID (generated when collapsing) 
  new_df_wo_TI <- collapsed_data %>%
mutate(Dt_correct = format(DateTime, "%Y-%m-%d %H:%M:%S"),
       Dt_correct = if_else(ID =="Aspa011_Rev17_IMG_0001", "2022-10-06 08:43:16", Dt_correct),
       Dt_correct = if_else(ID =="Aspa011_Rev17_IMG_0001", "2022-10-06 08:43:00", Dt_correct),
       Dt_correct = if_else(ID =="Aspa011_Rev17_IMG_0018", "2022-10-06 11:32:00", Dt_correct),
       Dt_correct = if_else(ID =="Aspa011_Rev17_IMG_0023", "2022-10-06 12:08:00", Dt_correct),
       Dt_correct = if_else(ID =="Aspa011_Rev17_IMG_0029", "2022-10-06 13:57:00", Dt_correct),
       Dt_correct = if_else(ID =="Aspa011_Rev17_IMG_0030", "2022-10-06 14:05:00", Dt_correct),
       Dt_correct = if_else(ID =="Aspa011_Rev17_IMG_0031", "2022-10-06 14:11:00", Dt_correct),
       Dt_correct = if_else(ID =="Aspa011_Rev17_IMG_0032", "2022-10-06 14:27:00", Dt_correct),
       Dt_correct = if_else(ID =="Aspa011_Rev18_IMG_0005", "2022-10-21 12:00:00", Dt_correct),
       Dt_correct = if_else(ID =="Aspa011_Rev09_IMG_0275", "2022-08-12 10:00:00", Dt_correct),
       Dt_correct = if_else(ID =="Aspa011_Rev18_IMG_0007", "2022-10-23 08:00:00", Dt_correct),
       Dt_correct = if_else(ID =="Aspa011_Rev17_IMG_0075", "2022-10-09 02:24:00", Dt_correct),
       Dt_correct = if_else(ID =="Aspa011_Rev18_IMG_0010", "2022-10-24 07:06:00", Dt_correct),
       Dt_correct = if_else(ID =="Aspa011_Rev17_IMG_0107", "2022-10-10 19:04:00", Dt_correct),
       Dt_correct = if_else(ID =="Aspa011_Rev17_IMG_0108", "2022-10-10 19:34:00", Dt_correct),
       Dt_correct = if_else(ID =="Aspa011_Rev18_IMG_0026", "2022-10-25 09:13:00", Dt_correct),
       Dt_correct = if_else(ID =="Aspa011_Rev18_IMG_0028", "2022-10-25 12:51:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev11_IMG_0002", "2022-08-25 12:49:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev10_IMG_0010", "2022-08-19 13:05:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev10_IMG_0029", "2022-08-20 14:35:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev10_IMG_0015", "2022-08-18 14:20:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev10_IMG_0016", "2022-08-18 16:00:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev10_IMG_0030", "2022-08-21 08:00:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev10_IMG_0034", "2022-08-19 13:05:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev11_IMG_0007", "2022-08-25 10:30:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev11_IMG_0008", "2022-08-25 10:45:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev11_IMG_0010", "2022-08-25 12:40:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev10_IMG_0066", "2022-08-20 14:56:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev10_IMG_0067", "2022-08-20 15:00:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev10_IMG_0066", "2022-08-20 14:56:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev10_IMG_0070", "2022-08-20 15:10:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev10_IMG_0072", "2022-08-20 15:20:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev10_IMG_0072", "2022-08-20 15:20:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev11_IMG_0016", "2022-08-27 17:05:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev11_IMG_0017", "2022-08-27 10:00:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev11_IMG_0017", "2022-08-27 10:00:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev11_IMG_0018", "2022-08-28 11:00:00", Dt_correct),
       Dt_correct = if_else(ID =="Aune016_Rev18_IMG_0107", "2022-12-26 17:20:00", Dt_correct),
       Dt_correct = if_else(ID =="Aune016_Rev18_IMG_0110", "2022-08-26 19:30:00", Dt_correct),
       Dt_correct = if_else(ID =="Aune016_Rev18_IMG_0111", "2022-08-26 20:45:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev10_IMG_0114", "2022-08-24 12:05:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev10_IMG_0116", "2022-08-24 15:15:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev10_IMG_0116", "2022-08-24 15:15:00", Dt_correct),
       Dt_correct = if_else(ID =="Aune016_Rev18_IMG_0118", "2022-08-24 15:15:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev10_IMG_0118", "2022-08-24 18:30:00", Dt_correct),
       Dt_correct = if_else(ID =="Aune016_Rev18_IMG_0129", "2022-12-26 11:40:00", Dt_correct),
       Dt_correct = if_else(ID =="Aune016_Rev18_IMG_0133", "2022-12-27 19:50:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev11_IMG_0004", "2022-08-25 07:30:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm002_Rev08_IMG_0093", "2022-08-10 17:15:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev11_IMG_0005", "2022-08-26 17:20:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev11_IMG_0012", "2022-08-27 10:10:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev11_IMG_0012", "2022-08-27 10:10:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev11_IMG_0013", "2022-08-27 11:00:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev12_IMG_0387", "2022-09-02 11:20:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev11_IMG_0023", "2022-08-28 13:10:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev11_IMG_0025", "2022-08-28 13:40:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev11_IMG_0029", "2022-08-28 15:30:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev11_IMG_0031", "2022-08-28 17:50:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev11_IMG_0037", "2022-08-29 15:30:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev11_IMG_0048", "2022-08-30 14:40:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev11_IMG_0049", "2022-08-30 16:00:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev11_IMG_0050", "2022-08-30 17:20:00", Dt_correct),
       Dt_correct = if_else(ID =="Aune016_Rev18_IMG_0156", "2023-01-01 00:30:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev11_IMG_0054", "2022-08-29 09:36:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev11_IMG_0055", "2022-08-30 10:30:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev11_IMG_0057", "2022-08-30 13:00:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev11_IMG_0058", "2022-08-30 14:30:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev11_IMG_0059", "2022-08-30 15:20:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm015_Rev11_IMG_0060", "2022-08-30 20:15:00", Dt_correct),
       Dt_correct = if_else(ID =="Aune016_Rev18_IMG_0318", "2023-01-11 17:50:00", Dt_correct),
       Dt_correct = if_else(ID =="Aune016_Rev18_IMG_0320", "2023-01-12 09:00:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0009", "2022-09-14 11:40:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0011", "2022-09-14 11:47:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0013", "2022-09-14 13:25:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0023", "2022-09-14 15:20:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0062", "2022-09-14 16:30:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0075", "2022-09-15 12:40:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0144", "2022-09-15 17:40:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0357", "2022-09-16 12:20:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0358", "2022-09-16 13:40:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0379", "2022-09-16 16:50:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0385", "2022-09-16 17:40:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0385", "2022-09-16 17:40:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0415", "2022-09-16 10:20:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0438", "2022-09-17 10:30:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0441", "2022-09-17 11:00:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0442", "2022-09-17 11:50:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0445", "2022-09-18 12:40:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0452", "2022-09-18 15:05:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0453", "2022-09-18 15:15:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0458", "2022-09-18 15:50:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0477", "2022-09-18 08:00:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0477", "2022-09-18 08:00:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0478", "2022-09-18 09:00:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0478", "2022-09-18 09:00:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0479", "2022-09-18 09:20:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0480", "2022-09-18 09:40:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0481", "2022-09-18 09:52:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0482", "2022-09-18 10:00:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0485", "2022-09-18 11:43:00", Dt_correct),
       Dt_correct = if_else(ID =="Rulm048_Rev14_IMG_0577", "2022-09-20 15:30:00", Dt_correct),
       Dt_correct = if_else(ID =="Aspa003_Rev09_IMG_0349", "2022-08-12 00:12:00", Dt_correct),
       Dt_correct = if_else(ID =="Aspa003_Rev10_IMG_0478", "2022-08-19 07:46:00", Dt_correct),
       Dt_correct = if_else(ID =="Aspa003_Rev10_IMG_0479", "2022-08-19 07:52:00", Dt_correct))

#write.csv2(new_df_wo_TI, file = "/Users/Pablo/Desktop/sumhal_yr2.csv", sep = ",")
```





## 2.3 SUMHAL_YR2_JOXY 
### 1. Load data                                            - 12,185 rows
In this script I´ll run the entire database creation script for Joxy_yr2 alone. This dataset was apart in other hard drive and have a different path and was not integrated in sumhal_yr2 script by error, so I´ll need to integrate it apart.
```{r }
joxy_yr2 <- read.csv2("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/Timelapse results Yr2/joxy_yr2_revised_c02.csv", sep=",") %>%  
  select(File, RelativePath, DateTime, Sp1, Count1, Behaviour, Sex, Sp2, Count2, Behaviour2, Sp3, Juvenile, Favourite, Obs) %>%
  mutate(plant = "joxy")
```

### 2. FILTER by behavior & eliminate empty rows            - 224 rows
```{r }
#Filter only animals and eating behavior 
joxy <- joxy_yr2 %>%
  filter(Sp1 != "") %>%
  filter(Sp1 != "?") %>%
  filter(Sp1 != "person") %>%
  filter(Sp1 != "empty") %>%
  filter(Behaviour == "eating" | Behaviour == "probably eating" | Behaviour == "searching for food")

#Change Count to the number of individuals  
joxy$Count1 [joxy$Count1 == 0 ] <- 1
joxy$Count2 <- ifelse(joxy$Sp2 != "", 1, 0)

```

### 3. Introduce VIDEO DURATION                             - 224 rows
```{r}
#In case want to skip this part
joxy_duration <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/duration/joxy_yr2_duration.csv")

#------------------------------ SKIP -------------------------
# Get a list of all files in LaCie
dir <- list.dirs ("/Volumes/LaCie/SUMHAL_YR2") #Lista de todos los directorios en el Rack
fil <- list.files(dir, full.names = T) #Lista de archivos con sus directorios
fil_clean <- fil[!file.info(fil)$isdir] #Lista de videos sin directorios raiz
  #write.csv(fil_clean, "/Users/PV/video_file_list.csv", sep = ",")

# Get a list of video files with eating events (eating videos)  
files_joxy <- str_c(joxy$RelativePath , joxy$File, sep ="/")  #Lista de videos eating
files_joxy_path <- file.path("/Volumes/LaCie", files_joxy) %>% #Lista de videos eating con su path
   str_replace_all("\\\\", "/") 

# Compare eating list to the entire file list (all files in GRAID) to detect missmatches  
fil_lost <- setdiff(files_joxy_path, fil_clean) #What is in my_list_2 that is not in my_list_1?
length(fil_lost)

# Extract video duration from eating list
info <- lapply(files_joxy_path, av_media_info)  #apply info extraction function to video list (Time demanding)
duration <- sapply(info, function(x){as.numeric(x[1])}) #Extract only video duration (position 1)

# Join duration to database  
joxy_duration <- cbind(joxy, duration)

    #write.csv(joxy_duration,"/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/joxy_yr2_duration.csv")
```

### 4. Create new columns                                   - 224 rows
  Plant_ID, Revision id and new DF
```{r}
## CLEAN THE DATA (Video level == Data non collapsed to 5 min)
#Split path in different columns  
clean_joxy <- data.frame(str_split_fixed(joxy_duration$RelativePath, pattern ="\\\\", 4))
colnames(clean_joxy) <- c("folder", "Plant_sp", "Rev", "Plant_ID")

Plant_ID <- data.frame(clean_joxy$Plant_ID) %>%
  rename(Plant_ID = clean_joxy.Plant_ID)

Rev <- data.frame (str_sub(clean_joxy$Rev, start = 6L, end = 10)) %>%
  rename(Rev = str_sub.clean_joxy.Rev..start...6L..end...10.)


new_joxy_yr2 <- joxy_duration %>%
  select(File, RelativePath, DateTime, Sp1, Behaviour, Count1, Sp2, Behaviour2, Count2, Sp3, plant, duration, Favourite, Obs) %>%
  cbind(Plant_ID, Rev) %>%
  mutate(ID = str_to_lower(str_c (str_sub(Plant_ID, start = 1L, end = 7), str_sub(Rev, start = 1L, end = 5), sep="_")))

```

### 5. Introduce SAMPLING EFFORT                            - 224 rows
  There is a code to automatize the extraction of video file information regarding to Sampling effort at deployment and upper level. The script is called videofile_batch_data_extraction and creates the video_yr2.csv document.  
```{r}
# Effort data (revision level)
video_yr2 <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/sampling_effort/sampling_effort_yr2/video_yr2.csv", sep=",")     #In MAcbook pro   

#Merge effort data (some) to new_dat (all)
joxy_effort <- left_join(new_joxy_yr2, effort_by_plant, by = "ID", keep = F) %>%
  mutate(Plant_ID = str_sub(Plant_ID, start = 1L, end = 7)) %>% 
  select(plant, Plant_ID, Revision_ID, Deployment_ID, File, DateTime, Sp1, Count1, Sp2, Count2, Behaviour, Behaviour2, duration, Days, Videos, Days.in.field, n_cam, Favourite, Obs, TI) %>%
  distinct() %>%  #Remove duplicated rows 
  filter(Sp1 != "empty") 

summary(joxy_effort$TI) #Number of Timestamp Issues

```

### 6. Introduce COORDINATES and LOCATION                   - 224 rows
```{r}
  deployments <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/sampling_effort/Deployments_yr2.csv", sep=";") 

deployments <- deployments %>%
    mutate(Camera_ID = str_sub (Deployment_ID, start = 9, end = 12)) %>%
    select(Deployment_ID, Long, Lat, Start, End, Days, Camera_ID, Timestamp_Issues)

Plant_ID <- deployments$Deployment_ID %>%
  str_remove_all ("_") %>%
  str_sub (start =1L, end = 7) %>%
  str_to_title()

deplo <- data.frame(cbind (deployments, Plant_ID) %>%
  group_by(Plant_ID) %>%
  summarise(Plant_ID = first(Plant_ID),
            Camera_ID = paste(Camera_ID, collapse = ', '),
            Long = first(Long),
            Lat = first(Lat)))
   

joxy_location <- left_join(joxy_effort, deplo, by = "Plant_ID", keep = FALSE) %>%
  mutate(Coexistence = FALSE)
```
### 7. Collapse joxy videos                                 - 79 rows
```{r}
#check but it looks fine to me today.
collapsed_joxy_yr2 <- joxy_location %>%
  group_by(Plant_ID, Revision_ID) %>%
  mutate(interval = as.POSIXct(round(as.numeric(as.POSIXct(DateTime)) / (60*5)) * (60*5), origin="1970-01-01")) %>%
  group_by(interval, Sp1) %>%
  summarize(File = paste(File, collapse=', '),
            Plant = first(plant),
            Plant_ID = first(Plant_ID),
            Revision_ID = first(Revision_ID),
            DateTime = first(DateTime),
            Species = paste(Sp1, collapse=', '),
            Behaviour = paste(Behaviour, collapse=', '),
            Coexistence = first(Coexistence),
            Deployment_ID = first(Deployment_ID),
            n_cam = first(n_cam),
            Videos = mean(Videos),
            Days = mean(Days),
            Days.in.field = first(Days.in.field),
            n = sum(Count1),
            duration = sum(duration),
            long = first(Long),
            lat = first(Lat)) %>%
  mutate(videos_events = sqrt(n*12),  # 5 minute interval, so 12 intervals per hour
         ID = str_c(Plant_ID, Revision_ID, str_sub(File, start = 1L, end = 8), sep = "_")) #To select later on Timestamp Issue fixing
  
  #write.csv2(collapsed_joxy_yr2, "/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/sumhal_joxy_yr2.csv", sep =",")
```





## 2.4 SUMHAL INDETERMINED
### 1. Load data 
Indetermined (determined with external help) videos results from TimeLapse (YR1 & YR2)
```{r}
sumhal_indet <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/Timelapse results Yr2/indetermined_videos_yr1_yr2.csv", sep = ",")

```

### 2. Create new columns 
with plant, plant_ID, revision and append it to the new df "sum_ind"
```{r}
Plant <- data.frame(str_split_fixed(sumhal_indet$RelativePath, "\\\\", 4)) %>%
  rename (folder = X1, plant =  X2, Rev = X3, Deploy = X4) %>%
  select (plant)  #Change plant name to short 


Plant_ID <- data.frame(str_split_fixed(sumhal_indet$RelativePath, "\\\\", 4)) %>%    #note that pbou121 = pbou012 and i correct it after merging sampling effort 
  rename (folder = X1, plant =  X2, Rev = X3, Deploy = X4) %>%
  select(Deploy) %>%
  mutate(Plant_ID = str_remove_all(Deploy, "_"),
         Plant_ID = str_sub (Plant_ID, start = 1L, end = 7))

#need to separate yr1 from 2 because revision has different structure (yr1 short and yr2 has also the name of the plant species)
Rev_yr1 <- data.frame(str_split_fixed(sumhal_indet$RelativePath, "\\\\", 4)) %>%
  rename (folder = X1, plant =  X2, Rev = X3, Deploy = X4) %>%
  filter(folder == "SUMHAL_YR1") %>%
  select(Rev) %>%
  mutate(Rev = str_replace_all(Rev, "Review", "Rev"),
         Rev = str_remove_all(Rev, "_"),
         Rev = str_sub(Rev, start = 1L, end = 4))
  

Rev_yr2 <- data.frame(str_split_fixed(sumhal_indet$RelativePath, "\\\\", 4)) %>%
  rename (folder = X1, plant =  X2, Rev = X3, Deploy = X4) %>%
  filter(folder == "SUMHAL_YR2") %>%
  select(Rev) %>%
  mutate(Rev = str_sub(Rev, start = 6L, end = 10 ))
  

Revision <- rbind(Rev_yr1, Rev_yr2) 

#Bind the created columns to the DB

sum_ind <- sumhal_indet %>%
  bind_cols(Plant, Plant_ID, Revision)

```

### 3. Include video duration from hard drive. 
Need to split Yr1 and Yr2 for different Relative paths (coming from different hard drives)
```{r}
#Just in case dont want to run the time demanding code.
indet_yr1_duration <- read.csv2 ("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/duration/indet_yr1_duration.csv", sep=",")

#----------------------  YR1  ------------------------------------------------- 
#Split Yr1     
indet_yr1 <- sum_ind %>%
  filter(str_detect (RelativePath, "SUMHAL_YR1"))

# Get a list of all files in GRAID 
dir_yr1 <- list.dirs ("/Volumes/G-RAID/SUMHAL") #Lista de todos los directorios en el Rack
fil_yr1 <- list.files(dir_yr1, full.names = T, recursive = TRUE, include.dirs = FALSE) #Lista de archivos con su path (sin directorios)
    #write.csv(fil_yr1, "/Users/Pablo/Desktop/video_file_list_yr1.csv", sep = ",")

# Get a list of selected files   
files_indet_yr1 <- str_c(indet_yr1$RelativePath, indet_yr1$File, sep ="/") %>%  #Lista de videos eating
  str_replace_all("\\\\", "/") %>%
  str_remove_all("SUMHAL_YR1/")

files_indet_yr1_path <- file.path ("/Volumes/G-RAID/SUMHAL", files_indet_yr1) #Lista de videos eating con su path


# Compare eating list to the entire file list (all files in GRAID) to detect missmatches  
fil_lost <- setdiff(files_indet_yr1_path, fil_yr1) #What is in my_list_2 that is not in my_list_1?
length(fil_lost)

# Extract video duration from eating list (have it already run in Results folder)
info <- lapply(files_indet_yr1_path, av_media_info)  #apply info extraction function to video list (Time demanding)
duration_yr1 <- sapply(info, function(x){as.numeric(x[1])}) #Extract only video duration (position 1)
DateTime_indet_yr1 <- 

# Join duration to database  
indet_yr1_duration <- cbind(indet_yr1, duration_yr1) %>%
  rename(duration = duration_yr1)

   # write.csv(indet_yr1_duration,"/Users/Pablo/Desktop/indet_yr1_duration.csv")


#----------------------  YR2  ------------------------------------------------- 
#Split Yr2
indet_yr2 <- sum_ind %>%
  filter(str_detect (RelativePath, "SUMHAL_YR2"))

# Get a list of all files in GRAID 
dir_yr2 <- list.dirs ("/Volumes/G-RAID-3/SUMHAL_YR2") #Lista de todos los directorios en el Rack
fil_yr2 <- list.files(dir_yr1, full.names = T, recursive = TRUE, include.dirs = FALSE) #Lista de archivos con su path (sin directorios)
    #write.csv(fil_yr2, "/Users/PV/Desktop/video_file_list_yr2.csv", sep = ",")

# Get a list of selected files   
files_indet_yr2 <- str_c(indet_yr2$RelativePath, indet_yr2$File, sep ="/") %>%  #Lista de videos eating
  str_replace_all("\\\\", "/") #change syntax for  
files_indet_yr2_path <- file.path ("/Volumes/G-RAID-3", files_indet_yr2) #Lista de videos eating con su path


# Compare eating list to the entire file list (all files in GRAID) to detect missmatches  
fil_lost <- setdiff(files_indet_yr2_path, fil_yr2) #What is in my_list_2 that is not in my_list_1?
length(fil_lost)

# Extract video duration from eating list (have it already run in Results folder)
info <- lapply(files_indet_yr2_path, av_media_info)  #apply info extraction function to video list (Time demanding)
duration_indet_yr2 <- sapply(info, function(x){as.numeric(x[1])}) #Extract only video duration (position 1)

# Join duration to database  
indet_yr2_duration <- cbind(indet_yr2, duration_indet_yr2) %>%
  rename(duration = duration_indet_yr2)

    #write.csv(indet_yr2_duration,"/Users/Pablo/Desktop/indet_yr2_duration.csv")

```

### 4. Extract video dates from drives (YR1 and YR2) 
dates are wrong from a TL issue. Video dates from the entire file set will be extracted and an external csv will be created for each year.
```{r }
#To skip this code for date extraction
sumhal_yr1_mtime <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/sumhal_times_from_hard_drives/sumhal_yr1_mtime.csv") %>%
  select(-X)%>%
  distinct()

sumhal_yr2_mtime <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/sumhal_times_from_hard_drives/sumhal_yr2_mtime.csv")

# ---------------------- SKIP --------------------------------
#----------------------- YR1 ----------------------------------
dir_yr1 <- list.dirs ("/Volumes/G-RAID/SUMHAL") #Lista de todos los directorios en el Rack
fil_yr1 <- list.files(dir_yr1, full.names = T, recursive = TRUE, include.dirs = FALSE) #Lista de archivos con su path (sin directorios)

#  `video_files` is the list of video file paths
video_files <- data.frame(fil_yr1) %>%
  rename(file_path = fil_yr1)%>%
  distinct()

# Create an empty list to store the creation dates
creation_dates <- list()

# Loop through each video file path
for (file_path in video_files) {
  # Get the file information
  file_info <- file.info(file_path)
  
  # Extract the creation date
  creation_date <- file_info$mtime
  
  # Append the creation date to the list
  creation_dates <- c(creation_dates, creation_date)
}

#mtime is the time of file creation (no ctime because is the creation of this code)
DateTime_yr1 <- file_info$mtime

#Join the new Dt colmn to Cor_1_duration
sumhal_yr1_mtime <- cbind(video_files, DateTime_yr1)

#write.csv(sumhal_yr1_mtime, "/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/sumhal_times_from_har_drives/sumhal_yr1_mtime.csv")

#----------------------- YR2 ----------------------------------

dir_yr2 <- list.dirs ("/Volumes/G-RAID-3/SUMHAL_YR2") #Lista de todos los directorios en el Rack
fil_yr2 <- list.files(dir_yr1, full.names = T, recursive = TRUE, include.dirs = FALSE) 
  #Lista de archivos con su path (sin directorios)

#  `video_files` is the list of video file paths
video_files <- data.frame(fil_yr2) %>%
  rename(file_path = fil_yr2) %>%
  distinct()

# Create an empty list to store the creation dates
creation_dates <- list()

# Loop through each video file path
for (file_path in video_files) {
  # Get the file information
  file_info <- file.info(file_path)
  
  # Extract the creation date
  creation_date <- file_info$mtime
  
  # Append the creation date to the list
  creation_dates <- c(creation_dates, creation_date)
}

#mtime is the time of file creation (no ctime because is the creation of this code)
DateTime_yr2 <- file_info$mtime

#Join the new Dt colmn to Cor_1_duration
sumhal_yr2_mtime <- cbind(video_files, DateTime_yr2)

#write.csv(sumhal_yr1_mtime, "/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/sumhal_times_from_hard_drives/sumhal_yr2_mtime.csv")


```  

### 5. Include CORRECT VIDEO DATES 
  I´ll join correct dates obtained in chunk 3 and do it for each year to maintain the consitency of the code
```{r }
#---------------------- YR1 -----------------------------
indet_yr1_to_merge <- indet_yr1_duration %>%
  mutate(file_path = str_c(RelativePath, File, sep = "/"),
         file_path = str_c("/Volumes/G-RAID/", file_path, sep=""),
         file_path = str_replace_all(file_path, "\\\\", "/"),
         file_path = str_remove_all(file_path, "_YR1")) 

indet_yr1_correct_time <- left_join(indet_yr1_to_merge, sumhal_yr1_mtime, by = "file_path") %>%
  mutate(DateTime = DateTime_yr1)


#---------------------- YR2 -----------------------------
str(indet_yr2_duration)
str(sumhal_yr2_mtime)

indet_yr2_to_merge <- indet_yr2_duration %>%
  mutate(file_path = str_c(RelativePath, File, sep = "/"),
         file_path = str_c("/Volumes/G-RAID-3/", file_path, sep=""),
         file_path = str_replace_all(file_path, "\\\\", "/"))
        

indet_yr2_correct_time <- left_join(indet_yr2_to_merge, sumhal_yr2_mtime, by = "file_path")

```

### 6. Introduce SAMPLING EFFORT (We will introduce sampling effort for each year separately)
```{r}
#To skip the entire chunk 
indet_yr1_effort <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/sampling_effort/indet_yr1_effort.csv", sep=";")
indet_yr2_effort <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/effort/indet_yr2_effort.csv")

#-----------------------SKIP------------------------
#---------------- YR1 --------------------
# Effort data (revision level) for yr1
  video_yr1 <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/sampling_effort/sampling_effort_yr1/Video_yr1.csv", sep=",")   

#Create an ID to merge data_yr1 with Sampling effort 
indet_yr1_dur <- indet_yr1_correct_time %>% 
  mutate(Rev = str_to_lower(Rev), 
         ID = str_c(Plant_ID, Rev, sep="_"),
         ID = str_to_lower(ID))

#In Sampling effort stack efforts for plants that had 2 cameras (i.e Olea or Arbutus)
plant_id_yr1 <- video_yr1$Deployment_ID %>%
  str_remove_all("_") %>%
  str_sub (start =1L, end = 7) %>%
  str_to_lower()

revision_id_yr1 <- str_remove_all(video_yr1$Revision_ID, "_") %>%
  str_to_lower()

effort_yr1 <- data.frame (video_yr1 %>% 
  mutate(ID = str_c(plant_id_yr1, revision_id_yr1, sep= "_"))%>%
  group_by(ID) %>%
  add_count(ID) %>%     # Add a column with the Count of number of??
  summarise(Deployment_ID = paste(Deployment_ID, collapse = ', '),
            Revision_ID = first(Revision_ID), 
            Videos = sum(Videos),
            Days = sum (Days),
            Days.in.field= sum (Days.in.field),
            n_cam = (n),
            TI = as.logical(sum(Timestamp_Issues, na.rm = TRUE))))

#Merge effort data (some) to new_dat (all)
indet_yr1_effort <- left_join(indet_yr1_dur, effort_yr1, by = "ID", keep = F) %>%
  select(plant, Plant_ID, Rev, Deployment_ID, File, DateTime, Sp1, Count1, Sp2, Count2, Behaviour, Behaviour2, duration_yr1, Days, Videos, Days.in.field, n_cam, Favourite, Obs, TI) %>%
  distinct() %>%
  rename(duration = duration_yr1)

#write.csv2(indet_yr1_effort, "/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/sampling_effort/indet_yr1_effort.csv")

#---------------- YR2 --------------------
video_yr2 <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/sampling_effort/sampling_effort_yr2/video_yr2.csv", sep=";")       
#Create an ID to merge data_yr2 with Sampling effort 
indet_yr2_dur <- indet_yr2_correct_time %>% 
  mutate(ID = str_c(Plant_ID, Rev, sep="_"),
         ID = str_to_lower(ID))

#In Sampling effort stack efforts for plants that had 2 cameras (i.e Olea or Arbutus)
plant_id_yr2 <- video_yr2$deployment %>%
  str_remove_all("_") %>%
  str_sub (start =1L, end = 7) %>%
  str_to_lower()

revision_id_yr2 <- str_remove_all(video_yr2$rev, "_") %>%
  str_to_lower()

effort_yr2 <- data.frame (video_yr2 %>% 
  mutate(ID = str_c(plant_id_yr2, revision_id_yr2, sep= "_"))%>%
  group_by(ID) %>%
  add_count(ID) %>%     # Add a column with the Count of number of??
  summarise(Deployment_ID = paste(deployment, collapse = ', '),
            Revision_ID = first(rev), 
            Videos = sum(num_files),
            Days = sum (num_days),
            Days.in.field= sum (Functioning.days),
            n_cam = (n),
            TI = as.logical(sum(Timestamp_Issues, na.rm = TRUE))))


#Merge effort data (some) to new_dat (all)
indet_yr2_effort <- left_join(indet_yr2_dur, effort_yr2, by = "ID", keep = F) %>%
  select(plant, Plant_ID, Rev, Deployment_ID, File, DateTime, Sp1, Count1, Sp2, Count2, Behaviour, Behaviour2, duration, Days, Videos, Days.in.field, n_cam, Favourite, Obs, TI) %>%
  distinct() 

#write.csv2(indet_yr1_effort, "/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/sampling_effort/indet_yr2_effort.csv")

```

### 7. Introduce COORDINATES and LOCATION
```{r}
coords <- read.csv("/Users/Pablo/Documents/GitHub/SUMHAL_WP5_fieldwork/Deployments.csv", sep = ";") %>%
  mutate(Plant_ID = str_remove_all(Deployment_ID, "_"),
         Plant_ID = str_sub(Plant_ID, start = 1L, end = 7),
         Plant_ID = str_to_lower(Plant_ID))
   
# -------------  YR1  ----------------------------
ind_yr1 <- indet_yr1_effort %>%
  mutate(Plant_ID = str_to_lower(Plant_ID),
         Plant_ID = str_replace (Plant_ID, "pbou121", "pbou012"))  #Change error in a Pbou name

indet_yr1_location <- left_join(ind_yr1, coords, by = "Plant_ID", keep = FALSE) 

# -------------  YR2  ----------------------------
deployments <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/sampling_effort/Deployments_yr2.csv", sep=";") %>%
  mutate(Plant_ID = str_sub(Deployment_ID, start = 1L, end = 7),
         Plant_ID = str_to_lower(Plant_ID))
         

ind_yr2 <- indet_yr2_effort %>%
  mutate(Plant_ID = str_to_lower(Plant_ID))

indet_yr2_location <- left_join(ind_yr2, deployments, by = "Plant_ID", keep = FALSE) 

write.csv(indet_yr1_location, "/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/location/indet_yr1_location.csv") #change yr1 -> yr2
```

### 8. Bind data from YR1 & YR2 and change count
```{r}
#Bind df
indetermined <- rbind(indet_yr1_location, indet_yr2_location)

#Change Count to the number of individuals  
indetermined$Count1 [indetermined$Count1 == 0 ] <- 1
indetermined$Count2 <- ifelse(indetermined$Sp2 != "", 1, 0)

```

### 9. Split entries with two species & filter for eating behaviour
  We will duplicate the entries where there are two species (SP2 = "non empty") and rename Sp1 with Sp2 name. The rest of the columns will maintain invariable (traceable through Plant_ID/Rev/File). A new column "Coexistence" will be created for searching events with more than one species. 
```{r}
# Rows with only one species 
rows_wo_sp2 <- indetermined %>%
 filter(is.na(Sp2) | nchar(Sp2) == 0) %>%
  mutate(Coexistence = FALSE)

# Rows with two species (maintaining Sp1)  #Note that there are two species, but not necesarily the sp2 is eating!!
new_rows_for_sp1 <- indetermined %>%
  filter(!is.na(Sp2) & nchar(Sp2) > 0) %>%
  mutate(Coexistence = TRUE)

# Rows with two species - maintaining Sp2 (Duplicate rows based on Sp2 and Behaviour2 conditions) 
new_rows_for_sp2 <- indetermined %>%
  filter(!is.na(Sp2) & nchar(Sp2) > 0 & Behaviour2 %in% c("eating", "probably eating", "searching for food")) %>%
  mutate(Sp1 = Sp2,
         Behaviour = Behaviour2,
         Coexistence = TRUE)

# Combine the original data with the duplicated rows
indeter <- bind_rows(rows_wo_sp2 , new_rows_for_sp1, new_rows_for_sp2) %>%
  select(!c (Sp2, Count2, Behaviour2)) %>%
   filter(Behaviour == "eating" | Behaviour == "probably eating" | Behaviour == "searching for food")

```

### 10. Collapse videos to 5 minutes intervals. 
    119 rows
```{r}
#Collapse videos 5 min
collapsed_indeter <- indeter %>%
  group_by(Plant_ID, Rev) %>%
  mutate(interval = as.POSIXct(round(as.numeric(as.POSIXct(DateTime)) / (60*5)) * (60*5), origin="1970-01-01")) %>%
  group_by(interval, Sp1) %>%
  summarize(File = paste(File, collapse=', '),
            Plant = first(plant),
            Plant_ID = first(Plant_ID),
            Revision_ID = first(Rev),
            DateTime = first(DateTime),
            Species = paste(Sp1, collapse=', '),
            Behaviour = paste(Behaviour, collapse=', '),
            Coexistence = first(Coexistence),
            Deployment_ID = paste(Deployment_ID.x, collapse =', '),
            n_cam = first(n_cam),
            Videos = mean(Videos),
            Days = mean(Days.x),
            Days.in.field = first(Days.in.field),
            n = sum(Count1),
            duration = sum(as.numeric(duration)),
            Long = first(Long),
            Lat = first(Lat)) %>%
  mutate(ID = str_c(Plant_ID, Revision_ID, File, sep = "_")) #To select later on Timestamp Issue fixing


#Fix inconsistent names 
fixed_collapsed_indet <- collapsed_indeter %>%
mutate(Plant = if_else(Plant == "Corema_2", "calb", Plant),
       Plant = if_else(Plant == "Rubus", "rulm", Plant),
       Plant = if_else(Plant == "Olea", "oeur", Plant),
       Plant = if_else(Plant == "Asparagus", "aspa", Plant),
       Plant = if_else(Plant == "Arbutus", "aune", Plant),
       Plant = if_else(Plant == "Myrtus", "mcom", Plant),
       Plant = if_else(Plant == "Smilax", "sasp", Plant),
       Plant_ID = if_else(Plant == "Calb", str_replace_all(Plant_ID, "a", "calb"), Plant_ID))

       
#write.csv2(fixed_collapsed_indet, "/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/sumhal_indet_yr1_yr2")
```


## 2.5 COREMA 1
### 1. Load data                                    - 2,794 rows
```{r }
corema_1 <-  read.csv2 ("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/Corema1/corema_1_results.csv", sep = ",") 
```

### 2.FILTER by behavior                            - 1,169 rows
    & eliminate empty & non animal videos. Also check for inconsistencies and create plant name column
```{r }
#Filter for species and behaviour, correct names and select columns.
cor_1 <-  corema_1 %>%
   filter(Sp1 != "person", Sp1 != "?", Sp1 != "--", Sp1 != "-.", Sp1 != "") %>%
   filter(Behaviour == "eating" | Behaviour == "probably eating" | Behaviour == "searching for food") %>%
   mutate(Sp1 = if_else(Sp1 == "orictolagus cuniculos", "orictolagus cuniculus", Sp1)) %>%
   select(File, RelativePath, DateTime, Sp1, Count1, Behaviour, Sex, Sp2, Count2, Behaviour2, Sp3, Juvenile, Favourite, Obs) %>%
   mutate(plant = "corema") 

#Change counts 
cor_1$Count1 [cor_1$Count1 == 0 ] <- 1
cor_1$Count2 <- ifelse(cor_1$Sp2 != "", 1, 0)
```

### 3. Include Video DURATION                       - 1,584 rows
```{r}
#For skipping this chunk 
cor_1_duration <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/duration/cor_1_duration.csv", sep =",")

#--------------------------------------------------------------------------  
# Get a list of all files in LaCie
dir <- list.dirs ("/Volumes/G-RAID/SUMHAL/Corema_1") #Lista de todos los directorios en el Rack
fil <- list.files(dir, full.names = T) #Lista de archivos con sus directorios
fil_clean <- fil[!file.info(fil)$isdir] #Lista de videos sin directorios raiz
  #write.csv(fil_clean, "/Users/PV/video_file_list.csv", sep = ",")

# Get a list of video files with eating events (eating videos)  
files_cor_1 <- str_c(cor_1$RelativePath , cor_1$File, sep ="/")  #Lista de videos eating
files_cor_1_path <- file.path("/Volumes/G-RAID/SUMHAL/Corema_1", files_cor_1) %>% #Lista de videos eating con su path
   str_replace_all("\\\\", "/") 

# Compare eating list to the entire file list (all files in GRAID) to detect missmatches  
fil_lost <- setdiff(files_cor_1_path, fil_clean) #What is in my_list_2 that is not in my_list_1?
length(fil_lost)

# Extract video duration from eating list
info <- lapply(files_cor_1_path, av_media_info)  #apply info extraction function to video list (Time demanding)
duration <- sapply(info, function(x){as.numeric(x[1])}) #Extract only video duration (position 1)

str(info)

# Join duration to database  
cor_1_duration <- cbind(cor_1, duration) %>%
  select(-DateTime)

    #write.csv(cor_1_duration,"/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/cor_1_duration.csv")
  
```

### 4. CORRECT DateTime COLUMN 
      In Corema_1 DateTime column was wrong (because of an bug in Timelapse) so I´ll need to extract         the DateTime column from files
```{r }
#  `video_files` is the list of video file paths
video_files <- data.frame(files_cor_1_path) %>%
  rename(file_path = files_cor_1_path)

# Create an empty list to store the creation dates
creation_dates <- list()

# Loop through each video file path
for (file_path in video_files) {
  # Get the file information
  file_info <- file.info(file_path)
  
  # Extract the creation date
  creation_date <- file_info$ctime
  
  # Append the creation date to the list
  creation_dates <- c(creation_dates, creation_date)
}

#mtime is the time of file creation (no ctime because is the creation of this code)
DateTime <- file_info$mtime

#Join the new Dt colmn to Cor_1_duration
cor_1_mtime <- cbind(cor_1_duration, DateTime)
```

### 5. Create new columns for later joins
```{r }
## CLEAN THE DATA (Video level == Data non collapsed to 5 min)
#Split path in different columns  
clean_cor_1 <- data.frame(str_split_fixed(cor_1_duration$RelativePath, pattern ="\\\\", 2)) %>%
  mutate(plant = "calb")
colnames(clean_cor_1) <- c("revision", "Plant_ID", "plant")

Plant_ID <- data.frame(clean_cor_1$Plant_ID) %>%
  rename(Plant_ID = clean_cor_1.Plant_ID) %>%
  mutate(Plant_ID = str_sub(Plant_ID, start = 1L, end = 3),
         Plant_ID = str_replace_all(Plant_ID, "A", "calb0"))

Plant_ID <- data.frame(clean_cor_1$Plant_ID) %>%
  rename(Plant_ID = clean_cor_1.Plant_ID) %>%
  mutate(Plant_ID = str_replace_all(Plant_ID, "A", "A0"),
         Plant_ID = str_sub(Plant_ID, -2),
         Plant_ID = str_c ("calb", Plant_ID, sep = ""))

Rev <- data.frame (str_sub(clean_cor_1$revision, start = 1L, end = 1)) %>%
  rename(Rev = str_sub.clean_cor_1.revision..start...1L..end...1.) %>%
  mutate(Rev = str_c("Rev0", Rev, sep = ""))

new_cor_1 <- cor_1_mtime %>%
  select(File, RelativePath, DateTime, Sp1, Behaviour, Count1, Sp2, Behaviour2, Count2, Sp3, plant, duration, Favourite, Obs) %>%
  cbind(Plant_ID, Rev) %>%
  mutate(ID = str_to_lower(str_c (str_sub(Plant_ID, start = 1L, end = 7), str_sub(Rev, start = 1L, end = 5), sep="_")))

#Change counts for Corema_1
new_cor_1$Count1 [new_cor_1$Count1 == 0 ] <- 1
new_cor_1$Count2 <- ifelse(new_cor_1$Sp2 != "", 1, 0)

```

### 6.Introduce SAMPLING EFFORT -  
```{r}
# Effort data for cor_1 (revision level)
video_cor_1 <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/sampling_effort/sampling_effort_yr1/Corema_1_dates.csv", sep=",")   

effort_cor_1 <- video_cor_1 %>%
  mutate(Plant_ID = str_sub(Plant_ID, -2),
         Plant_ID = str_c("calb", Plant_ID, sep = ""),
         Rev = str_replace_all(Rev, "Rev_", "rev_0"),
         Rev = str_remove_all(Rev, "_"),
         ID = str_c(Plant_ID, Rev, sep = "_"),
         n_cam = 1) %>%
        #This values come from an external source
  mutate(Days.in.field = case_when(Rev == "rev01" ~ 26,
                                   Rev == "rev02" ~ 14,
                                   Rev == "rev03" ~ 15,
                                   Rev == "rev04" ~ 20,
                                   Rev == "rev05" ~ 20,
                                   Rev == "rev06" ~ 30,
                                   Rev == "rev07" ~ 26,
                                   Rev == "rev08" ~ 12,
                                   TRUE ~ 0)) %>%
  rename(Days = num_days, Videos = num_files) %>% 
  select(subdir, ID, Rev, first_file_date, last_file_date, Videos, Days, Days.in.field, n_cam)

#Merge effort data (some) to new_dat (all)
cor_1_effort <- left_join(new_cor_1, effort_cor_1, by = "ID", keep = FALSE) %>%
  select(plant, Plant_ID, Rev.x, File, DateTime, Sp1, Count1, Sp2, Count2, Behaviour, Behaviour2, duration, Days, Videos, Days.in.field, n_cam, Favourite, Obs) %>%
  rename(Rev = Rev.x) %>%
  distinct() %>%  #Remove duplicated rows 
  filter(Sp1 != "empty") 

```

### 7. Introduce COORDINATES and LOCATION
```{r}
  coords_cor <- read.csv("/Users/Pablo/Documents/GitHub/SUMHAL_WP5_fieldwork/SUMHAL_fieldwork_Corema_1/corema_coords.csv", sep=",") %>%
  mutate(Plant_ID = str_replace_all(Name, "A","calb") )
 
   

cor_1_location <- left_join(cor_1_effort, coords_cor, by = "Plant_ID", keep = FALSE) %>%
  rename (Revision_ID = Rev)
  
```

### 8. Collapse videos to 5 minutes intervals.
```{r}
#Collapse videos 5 min
collapsed_corema_1 <- cor_1_location %>%
  group_by(Plant_ID, Revision_ID) %>%
  mutate(interval = as.POSIXct(round(as.numeric(as.POSIXct(DateTime)) / (60*5)) * (60*5), origin="1970-01-01")) %>%
  group_by(interval, Sp1) %>%
  summarize(File = paste(File, collapse=', '),
            Plant = first(plant),
            Plant_ID = first(Plant_ID),
            Revision_ID = first(Revision_ID),
            DateTime = first(DateTime),
            Species = paste(Sp1, collapse=', '),
            Behaviour = paste(Behaviour, collapse=', '),
            Coexistence = FALSE,
            Deployment_ID = NA,
            n_cam = first(n_cam),
            Videos = mean(Videos),
            Days = mean(Days),
            Days.in.field = first(Days.in.field),
            n = sum(Count1),
            duration = sum(duration),
            long = first(X),
            lat = first(Y)) %>%
  mutate(videos_events = sqrt(n*12),  # 5 minute interval, so 12 intervals per hour
         ID = str_c(Plant_ID, Revision_ID, File, sep = "_")) #To select later on Timestamp Issue fixing
  
kk <- collapsed_corema_1 %>%
  group_by(Revision_ID) %>%
  summarise(range(DateTime))

write.csv2(collapsed_corema_1, "/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/sumhal_corema_1.csv", sep =",")
```




## 2.6 PISTACIA LENTISCUS
### 1. Load data & bind different years                             - 4,563 rows
```{r}
#Dataset to integrate (Elenas Pistacia must be integrated to Sumhal data)
len_1 <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/Datos Elena - Pistacia/visits_cam_2018-19_sumhal.csv", sep = ";")
len_2 <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/Datos Elena - Pistacia/visits_cam_2019-20_sumhal.csv", sep = ";")

#Plen_coordinates
coord <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/Datos Elena - Pistacia/id_plant_coords_updated.csv",sep = ";")

#Bind 2018-19 and 2019-20 field data in a unique dataset (n = 4563 obs).
len_1 <- len_1[1:14]
len_2 <- len_2[1:14]
  len_2$duration <- str_sub(len_2$duration, start = 4, end =7) #need to eliminate %H from %H:%M:%OS for consistency with len_1

  len <- rbind(len_1, len_2)
```
  
### 2. Fix SAMPLING EFFORT for consistency with Sumhal              - 4,563 rows
```{r}  
#Esfuerzo de muestreo: Hay que transformar el tiempo en formato "%H:%M:%OS" en horas decimales que es como está en la BD "final_data"
hora_decimal <- sapply(strsplit(len$recording_time, ":"),
                       function(x) {
                         x <- as.numeric(x)
                         x[1]+(x[2]/60)+(x[3]/3600)
                          })

len <- cbind(len, hora_decimal)

# The duration of the eating event will be a proxy for Interaction Intensity
duration_sec <- sapply(strsplit(len$duration, ":"),
              function(x) {
                x <- as.numeric(x)
                (x[1]*60)+(x[2])
              })

len <- cbind(len, duration_sec)
```

### 3. FILTER eating behavior & merge COORDINATES                   - 1,164 rows
     Also change colnames to match those in Sumhal df
```{r} 
#Format Pistacia dataset to phototrapping dataset
plen <- len %>%
  mutate (yr = str_sub(date, start = 1, end =  4), #Create DateTime in format %Y:%M:%D %H:%M:%S
          month = str_sub(date, start = 5, end =  6),
          day = str_sub(date, start = 7, end =  8),
          new_date = str_c(yr, month, day, sep = "-"),
          DateTime = str_c(new_date, real_arrival, sep = " ")) %>%  #Hour starts in the animal arrival
  merge (coord, by.x = "plant_id", by.y = "id_plant") %>%           #Merge coordinates from coord dataset
  mutate (behav = str_replace_all(behav, "not_feeding", "not_feed"), #Change name "feeding" to "feed" for the next step  
          feed = str_detect(behav,  "feeding")) %>%                  #Create column feeding for selection (includes probably feeding)
  filter (feed == "TRUE") %>%                                       #Filter only sequences with animal eating
  mutate (file = video_name,
          path = file.path("/Helen_file", video_name),
          plant = "plen",
          ncam = 1,
          Videos = NA,
          esf_muest = hora_decimal/12,                        #convert decimal hours to Days. (Previously recording_time converted to decimal above)
          duration = duration_sec,                             # visit duration in seconds (maybe should divide by 10 to maintain consistency with the rest of phototrapping data (duration in seconds converted above)
          TimestampIssues = NA,
          Sp1 = str_to_lower(spp),
          plant_id = str_c("Plen", plant_id))%>%
  select (plant, plant_id, DateTime, Sp1, behav, ncam, Videos, esf_muest, duration, TimestampIssues, X_coord, Y_coord, path)%>%   #Maybe select some other columns
  rename (plant_ID = plant_id, behaviour = behav, Days = esf_muest, duration = duration, Long = X_coord, Lat = Y_coord)


#write.csv(plen, "/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/plen_adapted.csv")
```



## 2.7 JUNIPERUS PHOENICIA
### 1. Load data                                        - 2,671 rows
```{r}
#Juniperus data
Juniper <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/Datos Jorge - Juniperus/Juniperus_phoenicea_CAMERATRAPS.csv", sep = ",")
sampling_effort <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/Datos Jorge - Juniperus/sampling_effort_CT_JP.csv", sep= ",")
coords <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/Datos Jorge - Juniperus/Juniper_coord.csv", sep=";")

```

### 2. Merge COORDINATES                                - 2,593 rows
```{r}
# Join coord X, Y and Sampling effort to Juniper data
coord <- coords %>%
  select(x_lat, y_long, tree) %>% #select only location and name from all individuals
  rename(ind = tree, long = y_long, lat = x_lat)%>%
  mutate(lat = str_c(substr(as.character(lat), 1, 2), ".", substr(as.character(lat), 3, nchar(as.character(lat)))), #introduce a point for decimal degrees in lat and long
         long = str_c(substr(as.character(long), 1, 2), ".", substr(as.character(long), 3, nchar(as.character(long)))))


jpho_coord <- left_join(Juniper, coord, by="ind") %>%  # Join coordenates to data
                  distinct() %>%
                  mutate(ID = str_c(ind, str_c("rev", test, sep = ""), sep = "_"))
```

### 3. Merge SAMPLING EFFORT                            - 2,593 rows
```{r}
#Modify sampling effort ... gather for columns "test"
eff <- sampling_effort %>%     
  gather(key = "rev", value = "Day", 3:37) %>%    
  filter(Day > 0) %>%
  mutate(rev = str_replace_all(rev, "test", "rev")) %>%
  mutate(ID = str_c(ind, rev, sep = "_")) %>%
  select(ID, Day)
  

jpho <- left_join(jpho_coord, eff, by="ID", keep=F) %>%  #Join effort to data
        distinct()
```

### 4. FILTER for behavior                              - 781 rows
      and ADAPT COLNAMES to sumhal df 
```{r}
jp <- jpho %>%
  mutate(DateTime = str_c(date,time, sep=" "),
        File = str_c(code, type, sep="."),
        Sp1 = str_c(genus, species, sep=" "),
        Sp1 = str_to_lower(Sp1),
        duration = 10) %>%
  select(DateTime, File, Sp1, ind, behaviour, lat, long, Day, duration,ID) %>%
  rename(plant_id = ind, Long = long, Lat = lat) %>%
  filter(behaviour == "eating_jp_cone" | behaviour =="feeding_or looking_for_jp"| behaviour =="jp_predation") #Select only Eating Events 

```

### 5.Change DateTime for consistency with Sumhal df    - 841 rows
```{r}
#1. Arrange datetime for consistency with data2
    #1.1 Split date in 3 columns and give consistency
date_split <- as.data.frame(str_split_fixed (jpho$date, "/", 3)) %>%
  rename(Day = V1, Month = V2, Year = V3) %>%
  mutate(Day = str_pad(Day, width = 2, side = "left", pad = "0"),  #Add 0 only to values with one digit
         Month = str_pad(Month, width = 2, side = "left", pad = "0"),
         Year = str_c("20", Year, sep = ""),
         Year = str_replace_all(Year, "202019", "2019"))


    #1.2 Split time in 3 columns and give consistency
time <- as.data.frame(str_split_fixed (jpho$time, ":", 3)) %>%
  rename(hour = V1, min = V2, sec = V3) %>%
  mutate(hour = str_pad(hour, width = 2, side = "left", pad = "0"), #Add 0 only to values with one digit
         time = str_c(hour, min, sec, sep = ":")) %>%
  select(time)
    
      #1.3 Join the columns (reordered)
date_jpho <- date_split %>%
  mutate(Date = str_c(Year, Month, Day, sep = "-"),
         DateTime = str_c (Date, time$time, sep = " ")) %>%
  select (DateTime)

      #1.4 Join DateTime to jpho df and adapt colnames it to Sumhal df 
jp <- cbind(jpho, date_jpho) %>%
  mutate(File = str_c(code, type, sep="."),
         Sp1 = str_c(genus, species, sep=" "),
         Sp1 = str_to_lower(Sp1),
         duration = 10,
         Coexistence = NA,
         n_cam = 1,
         Days.in.field = NA,
         Plant = "jpho") %>%
  select(DateTime, File, Sp1, ind, behaviour, lat, long, Day, duration, Days.in.field, File, ID, n_cam, Plant, Coexistence) %>%
  rename(Behaviour = behaviour, Plant_ID = ind, Days = Day) %>%
  filter(Behaviour == "eating_jp_cone" | Behaviour == "eating_jp_leaves"| Behaviour =="feeding_or looking_for_jp"| Behaviour =="jp_predation") #Select only Eating Events )

```

### 6. Collapse to 5 minutes events                     - 812 rows
```{r}
jp$DateTime <- ymd_hms(jp$DateTime) # Make date and time vector understandable in R  

collapsed_jp <- jp %>%
  group_by(ID) %>%
  mutate(interval = as.POSIXct(round(as.numeric(as.POSIXct(DateTime)) / (60*5)) * (60*5), origin="1970-01-01")) %>%
  group_by(interval, Sp1) %>%
  summarize(Plant = first(Plant),
            Plant_ID = first(Plant_ID),
            DateTime = first(DateTime),
            Sp1 = first(Sp1),
            Behaviour = paste(Behaviour, collapse=', '),
            Coexistence = first(Coexistence),
            n_cam = first(n_cam),
            Days = first(Days),
            Days.in.field = NA,
            duration = sum(duration),
            long = first(long),
            lat = first(lat),
            ID = first(ID)) %>% 
  data.frame() %>%
  select(-interval)

#write.csv(collapsed_jp, "/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/jpho_adated.csv")

```









# 3. DATA MERGING
## 3.1 Load data 
  We read from csv the datasets generated in the "data unification" script above to skip running the entire code again.
```{r }
#3,053 rows
sumhal_yr1 <- read.csv2 ("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/sumhal_yr1.csv", sep = ";") #From Database_creation_sumhal_yr1

#5,304 rows
sumhal_yr2 <-  read.csv2 ("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/sumhal_yr2.csv", sep = ";")  #From Database_creation_sumhal_yr2

#79 rows
joxy_yr2 <- read.csv2 ("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/sumhal_joxy_yr2.csv", sep = ";") #From Database_creation_sumhal_joxy_yr1 (I not integrated in sumhal yr2 because it was in a different container and have some different issues)

#119 rows
sumhal_indet <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/sumhal_indet_yr1_yr2", sep = ";") #From Database_creation_indetermined

#721 rows
corema_1 <-  read.csv ("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/sumhal_corema_1.csv", sep =";")  #From script Database_creation_corema1 from the revised videos in Timelapse with Sumhal protocol (not Frucore) and already selected eating events
 
# 1,164 rows
plen <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/plen_adapted.csv") 
 
# 812 rows
jpho <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/jpho_adapted.csv") 
```

## 3.2 Integrate SUMHAL_YR1 & Yr2 & Joxy_Yr2                - 8,326 rows
  Here we filter/select data & correct plant names for consistency & merge sumhal Yr1 & Yr2 & joxy_Yr2 into a SUMHAL df
```{r }
s_yr1 <- sumhal_yr1 %>%
  mutate(Behaviour = first(Behaviour)) %>%
  filter(Plant != "pinus" ) %>%   #Eliminate Pinus from the db as it is not fleshy fruit
  select(Sp1, Plant, Plant_ID, Revision_ID, DateTime, Behaviour, Coexistence, n_cam, Days, Days.in.field, duration, ID, long, lat) %>%
  mutate(Plant = if_else ( Plant == "pyrus"      , "pbou" , Plant), #Change plant names for consistency
        Plant = if_else ( Plant == "corema_2"   , "calb" , Plant),
        Plant = if_else ( Plant == "asparagus"  , "aspa" , Plant),
        Plant = if_else ( Plant == "rubus"      , "rulm" , Plant),
        Plant = if_else ( Plant == "olea"       , "oeur" , Plant),
        Plant = if_else ( Plant == "rubia"      , "rper" , Plant),
        Plant = if_else ( Plant == "arbutus"    , "aune" , Plant),
        Plant = if_else ( Plant == "smilax"     , "sasp" , Plant),
        Plant = if_else ( Plant == "myrtus"     , "mcom" , Plant),
        Plant_ID = ifelse(Plant == "calb", str_replace(Plant_ID, "a", "calb"), Plant_ID), #Change names only for Corema
        DateTime = as.POSIXct(DateTime))
        
s_yr2 <- sumhal_yr2 %>%
  mutate(ID = str_c(Plant_ID, Revision, File, sep = "_"),
         Dt_correct = as.POSIXct(Dt_correct)) %>%
  select(Sp1, Plant, Plant_ID, Dt_correct, Behaviour, Coexistence, n_cam, Days, Days.in.field, Duration, ID, long, lat) %>%
  rename(DateTime = Dt_correct, duration = Duration) %>%
  mutate(Plant_ID = str_to_lower(Plant_ID), 
         DateTime = as.POSIXct(DateTime))
  

j_yr2 <- joxy_yr2 %>%
  mutate(ID = str_c(Plant_ID, Revision_ID, File, sep = "_")) %>%
  select(Sp1, Plant, Plant_ID, Revision_ID, DateTime, Behaviour, Coexistence, n_cam, Days, Days.in.field, duration, ID, long, lat) %>%
  mutate(Plant_ID = str_to_lower(Plant_ID),
         DateTime = as.POSIXct(DateTime))

sumhal <- rbind(s_yr1, s_yr2, j_yr2)   #Here Sumhal data are consistent and joinned in one df 
```

## 3.3 Integrate COREMA_1                                   - 721 rows
```{r }
cor_1 <- corema_1 %>%
  mutate(Plant = "calb",
        Coexistence = FALSE,
        ID = str_sub(ID, start = 1L, end = 12),
        DateTime = as.POSIXct(DateTime)) %>%
  select(Sp1, Plant, Plant_ID, Revision_ID, DateTime, Behaviour, Coexistence, n_cam, Days, Days.in.field, duration, ID, lat, long)

  
#Join SUMHAL (8,326 row) df to COREMA_1 (721 rows) DF == 9,047 rows 

data1 <- rbind(sumhal, cor_1)

```

## 3.4 Integrate PISTACIA LENTISCUS                         - 1,164 rows
```{r }
plen <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/plen_adapted.csv") %>%
  mutate(Days.in.field = NA,
         ID = NA,
         Coexistence = FALSE,
         DateTime = as.POSIXct(DateTime),
         plant_ID = str_to_lower(plant_ID)) %>%
  select(plant, plant_ID, DateTime, Sp1, behaviour, ncam, Days, Days.in.field, duration, ID, Lat, Long, Coexistence ) %>%
  rename(Plant = plant, Plant_ID = plant_ID, Behaviour = behaviour, n_cam = ncam, lat = Lat, long = Long)


data2 <- rbind(data1, plen)

```

## 3.5 Integrate JUNIPERUS PHOENICEA                        - 812 rows
```{r }
jpho <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/jpho_adapted.csv") %>%
  select(-X, Plant) %>%
  mutate(Plant = "jpho", #Need to change plant name (due to a mistake in the other db)
         Plant_ID = str_replace_all(Plant_ID, "C", "jpho"), #Change plant ID for consistency with the rest of the data
         Plant_ID = str_replace_all(Plant_ID, "M", "jpho"),
         Plant_ID = str_replace_all(Plant_ID, "O", "jpho"),
         Sp1 = str_replace_all(Sp1, "rodent", "apode-mus"),
         DateTime = as.POSIXct(DateTime)) %>%
  filter(!is.na(DateTime))
  

data3 <- rbind(data2, jpho)
```

## 3.6 Integrate INDETERMINED VIDEOS 
  Here we integrate videos that were we had doubts on the species determination and were determined later in the process (for Sumhal yr1 and yr2)
```{r }
#data3 to data4
sumhal_ind <- sumhal_indet %>%
  mutate(DateTime = as.POSIXct(DateTime),
         Plant = str_to_lower(Plant))%>%
  rename(lat = Lat , long = Long) %>%
  select(Behaviour, Coexistence, DateTime, Days, Days.in.field, duration, ID, lat, long, n_cam, Plant, Plant_ID, Sp1)

data4 <- rbind(data3, sumhal_ind)

```

## 3.7 Include SAMPLING EFFORT by plant species and induvidual
  Here we create new columns with sampling effort by Plant and plant_ID (from a different script called Sampling effort) - data5 has both merged efforts
```{r }
effort_species <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/sampling_effort/effort_species.csv") %>%
  mutate(Plant = str_to_lower(Plant))%>%
  select(-X)

effort_individual <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/sampling_effort/effort_individual_level.csv") %>%
  mutate(Plant_ID = str_to_lower(Plant_ID))%>%
  select(-X)

#Join effort by plant species 
data4_eff_sp <- left_join(data4, effort_species, by = "Plant", keep = FALSE)

#Join effort by plant individuals (data 5 has both efforts merged)
data5<- left_join(data4_eff_sp, effort_individual, by = "Plant_ID", keep = FALSE)

```

## 3.8 CORRECT COORDINATES 
  Here we join new coordinates to the df because there were some inconsistencies in format. 
```{r }
  <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Results/temp_results/coordinates/coordinates_13_spp.csv")

data6 <- left_join(data5, coord_13sp, by = "Plant_ID") %>%
  select(-long, -lat, -X)


```

## 3.9 Create the FINAL DF 
  Here we check and correct mistaken names for Sp1 & Plant_ID & create new column with mammal/bird
```{r }
# -------  SUMMARY  -----------
# 10,837 rows independent interactions where animals had foraging behaviours in the focus plant
# 62 animal species 
# 13 plant species (306 individuos)

#Correct mistaken names
data6$Sp1 [data6$Sp1 == "chloris cloris" ] <- "chloris chloris" 
data6$Sp1 [data6$Sp1 == "cyanistes caerulius" ] <- "cyanistes caeruleus" 
data6$Sp1 [data6$Sp1 == "cyanopica cyanus" ] <- "cyanopica cooki" 
data6$Sp1 [data6$Sp1 == "fringilia coelebs" ] <- "fringilla coelebs" 
data6$Sp1 [data6$Sp1 == "lanius excubitor" ] <- "lanius meridionalis" 
data6$Sp1 [data6$Sp1 == "orictolagus cuniculus" ] <- "oryctolagus cuniculus" 
data6$Sp1 [data6$Sp1 == "rodent rodent" ] <- "rodent" 
data6$Sp1 [data6$Sp1 == "sylvia sp." ] <- "sylvia sp" 
data6$Sp1 [data6$Sp1 == "unknown unknown" ] <- "unknown" 
data6$Sp1 [data6$Sp1 == "unkown" ] <- "unknown" 
data6$Sp1 [data6$Sp1 == "x" ] <- "unknown" 
data6$Sp1 [data6$Sp1 == "?" ] <- "unknown"
data6$Sp1 [data6$Sp1 == "apode-mus apode-mus" ] <- "apode-mus" 
data6$Plant_ID [data6$Plant_ID == "a03" ] <- "calb03" 
data6$Plant_ID [data6$Plant_ID == "a07" ] <- "calb07" 
data6$Plant_ID [data6$Plant_ID == "a10" ] <- "calb10" 
data6$Plant_ID [data6$Plant_ID == "oeur20_" ] <- "oeur020" 


#Create a final dataframe filtering non interesting species and column Mamal/bird and correct Timestamp Issues (change those asparagus that have events  pm to am)
data7 <- data6 %>%
  filter (Sp1 != "bos taurus",
          Sp1 != "equus caballus",
          Sp1 != "unknown", 
          Sp1 != "athene noctua") %>%
  mutate(SpeciesType = ifelse(grepl("cervus|sus|meles|vulpes|dama|apode-mus|oryctolagus|genetta|herpestes|lepus|rattus", data7$Sp1, ignore.case = TRUE), "Mammal", "Bird"),
         DateTime = case_when(SpeciesType == "Bird" & hour(DateTime) >= 21 & hour(DateTime) <= 23 ~ DateTime + hours(12), TRUE ~ DateTime),
         DateTime = case_when(SpeciesType == "Bird" & hour(DateTime) >= 0 & hour(DateTime) <= 6 ~ DateTime + hours(12), TRUE ~ DateTime)) %>% #Change all bird species that are eating during the night 12 hours (because most of those cases seem to be a am-pm issue)
  filter(DateTime != is.na(DateTime))

#write.csv(data7, "/Users/Pablo/Desktop/sumhal_data.csv")
```



## 3.10 Integrate TRAITS?
        Here we will merge traits for animal and plant species. 
```{r }
#Load and select traits for plants and animals
plant_traits <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Traits/Plant_traits_Frubase.csv", sep=";") %>%
  mutate(cod = str_replace_all(cod, "OQUA", "OLAN"))



bird_list <- data7 

mammal_list <- data7 %>% 
  filter(SpeciesType == "Mammal") %>%
  distinct(Sp1) %>%
  mutate(Sp1 = str_to_sentence(Sp1)) %>%
  as.list()

mammal_traits <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Traits/Mammal_traits_Eltontraits.csv")

bird_traits <- read.csv("/Users/Pablo/Documents/GitHub/Animal-detection-cameratrap/Traits/Bird_trait_Eltontraits.csv")



```

# 4. PRELIMINARY ANALYSIS AND FIGURES
## 4.1 Summary RESULTS
```{r }
#Number of independent events by animal species (~ abundance of eating)
data7 %>%
  count(Sp1, sort=T)

#Summary by plant species 
data7 %>%
  group_by(Plant)%>%
  summarise(n()) %>%
  data.frame() %>%
  arrange(desc(n..)) 

#Summary sampling effort by plant
data7 %>%
  group_by(Plant)%>%
  summarise(effort_pl = first(effort_pl_sp)) %>%
  data.frame() %>%
  arrange(desc(effort_pl)) 

#Events by animal type (mamal/bird)
data4 %>%
  count(mammal, sort=T)

#Events by plant individuals
ind_plant_level <- data.frame(data5 %>%
  group_by(Plant_ID, Sp1) %>%
  summarise(n=n()))%>%
  arrange(desc(n))

```

## 4.2 first NETWORKS
```{r }
#Adjacency matrix for plants individuals and animal species. 
    matrix <- xtabs(~Plant + Sp1, data7) #adjacency matrix
    plotweb(matrix, labsize = 0.5, low.lablength = 12, text.rot = 90) 
    visweb(matrix)
 
#------------------------------------------------------------------------------------------------
#Compare matrix for mammals vs birds
mammal <- data7 %>%
     filter(SpeciesType == "Mammal")
bird <- data7 %>%
     filter(SpeciesType == "Bird") 
    
mammal_mat <-  xtabs(~Plant + Sp1, mammal)
bird_mat <- xtabs(~Plant + Sp1, bird)
     

plotweb(mammal_mat, labsize = 1, text.rot = 90) 
plotweb(bird_mat, labsize = 0.8, text.rot = 90) 
visweb(mammal_mat, labsize = 1.5) 
visweb(bird_mat,  labsize = 1.9) 
```

## 4.3 DOUBLE AXIS chart
   Sampling effort vs number of interactions charts
   Here we ant to create a sampling effort bar chart for SPECIES and for individual plants
```{r }
#df with sampling effort by plant and number of interactions
df <- data7 %>%
  select(Plant, effort_pl_sp) %>%
  group_by(Plant)%>%
  summarise(eff = first(effort_pl_sp),
            n= n())

  # Convert 'Plant' variable to a factor for correct ordering in the plot
df$Plant <- factor(df$Plant)

  # Create a plot for sampling effort by plant species
ggplot(df, aes(x = Plant, y = eff, fill = Plant)) +
  geom_bar(stat = "identity") +
  labs(x = "Plant", y = "Effort", fill = "Plant") +
  theme_bw()
  # Create a plot for number of interactions by plant species
ggplot(df, aes(x = Plant, y = n, fill = Plant)) +
  geom_bar(stat = "identity") +
  labs(x = "Plant", y = "numer of interactions", fill = "Plant") +
  theme_bw()

#PLOT TOGETHER sampling effort and number of interactions
#Scale factor
scaleFactor <- max(df$eff) / max(df$n)

#Plot
ggplot(df, aes(x = fct_reorder(Plant, desc(n)),  width=.4)) +
  geom_col(aes(y=eff), fill="powderblue", position = position_nudge(x = -.4)) +
  geom_col(aes(y=n * scaleFactor), fill="indianred2") +
  scale_y_continuous(name="Sampling effort", sec.axis=sec_axis(~./scaleFactor, name="Number of interactions")) +
  xlab("Plant species") + 
  theme(
    axis.title.y.left=element_text(color="deepskyblue4"),
    axis.text.y.left=element_text(color="deepskyblue4"),
    axis.title.y.right=element_text(color="firebrick3"),
    axis.text.y.right=element_text(color="firebrick3"),
    panel.background = element_blank(),
    axis.line = element_line(colour = "black")) 

```

## 4.4 RECORDINGS by species 
Figure Animal Species recorded  x = number of records per species y = species names
```{r }
mammal <- data7 %>%
     filter(SpeciesType == "Mammal") %>%
  group_by(Sp1) %>%
  summarise(n(),
            SpeciesType = first(SpeciesType)) %>%
  data.frame()%>%
  mutate(Sp1 = str_to_sentence(Sp1))%>%
  arrange(desc(n..)) 

bird <- data7 %>%
     filter(SpeciesType == "Bird") %>%
  group_by(Sp1)%>%
  summarise(n(),
            SpeciesType = first(SpeciesType)) %>%
  data.frame()%>%
  mutate(Sp1 = str_to_sentence(Sp1))%>%
  #filter(n.. > 10)%>%
  arrange(desc(n..))

#bind mammals and birds and order each group by species Type 
df <- rbind (mammal, bird)
df$Sp1 <- factor(df$Sp1, levels = df$Sp1[order(df$n..)])

ggplot(df, aes(x = n.., y = fct_reorder(Sp1, SpeciesType), fill = SpeciesType)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Number of Events", y = "Species", fill = "Type") +
  theme(axis.text.y = element_text(size = 5),
        panel.background = element_blank(),
        axis.line = element_line(colour = "black"))
   

```

## 4.5 ACTIVITY patterns
      Preliminary script and Denisty plots for activity patterns in Doñana frugivores. 
```{r }
library(activity)
library(camtrapR)
library(overlap)
library(astroFns)

#1.Convert the data for later analysis 
  # Convert time into radians (Check dates for timestamp issues). 
radians <- gettime(data7$DateTime, format= "%Y-%m-%d %H:%M:%S", scale = "radian" )
date <- as.POSIXct(data7$DateTime, format = "%Y-%m-%d %H:%M:%S") #Date as.POSIXct
time <- date %>%
  str_sub (start =12L, end = 20)
  #solartime() transforms clock time to solar time anchored to sun rise and sunset times for a given location.
solartime <- solartime(date, lat= 37.0000000 , long= -6.3333300,tz= 2  ,"%Y-%m-%d %H:%M:%S")
solar <- solartime$solar #Create new column with solar time
solar_input <- solartime$input
  #Dataset for analizing activity patterns 
act_data <- data7 %>%
  cbind(radians, date, solar, solar_input, time) %>%
  select(Sp1, Plant,DateTime, date, Behaviour, solar, time)

#2. Analysis

  #2.1 Phitocentric (by plant species)
    #Construct df for each species
aspa <- act_data %>%subset(Plant == "aspa")
aune <- act_data %>%subset(Plant == "aune")
calb <- act_data %>%subset(Plant == "calb")
joxy <- act_data %>%subset(Plant == "joxy")
mcom <- act_data %>%subset(Plant == "mcom")
oeur <- act_data %>%subset(Plant == "oeur")
olan <- act_data %>%subset(Plant == "olan")
pbou <- act_data %>%subset(Plant == "pbou")
rper <- act_data %>%subset(Plant == "rper")
rulm <- act_data %>%subset(Plant == "rulm")
sasp <- act_data %>%subset(Plant == "sasp")
plen <- act_data %>%subset(Plant == "plen")

      #ASPARAGUS (Sylvias & cervus/rabbit)
aspa%>%group_by(Sp1)%>%summarise(n())

par(mfrow = c(2,2))
activityOverlap (recordTable = aspa, speciesA    = "sylvia melanocephala" , speciesB = "sylvia communis", plotR = TRUE, linewidth = c(2.5, 2.5), 
                 linetype = c(1,1), linecol = c("#4682B4", "#D7C26F"), olapcol = "#FFFFE0",
                 add.rug     = TRUE, speciesCol = "Sp1", recordDateTimeCol = "date",  recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", addLegend = FALSE)

activityOverlap (recordTable = rulm, speciesA    = "sylvia melanocephala" , speciesB = "sylvia communis", plotR = TRUE, linewidth = c(2.5, 2.5), 
                 linetype = c(1,1), linecol = c("#4682B4", "#D7C26F"), olapcol = "#FFFFE0",
                 add.rug     = TRUE, speciesCol = "Sp1", recordDateTimeCol = "date",  recordDateTimeFormat = "%Y-%m-%d %H:%M:%S",addLegend = FALSE)

activityOverlap (recordTable = pbou, speciesA    = "vulpes vulpes" , speciesB = "sus scrofa", plotR = TRUE, linewidth = c(2.5, 2.5), 
                 linetype = c(1,1), linecol = c("#4682B4", "#D7C26F"), olapcol = "#FFFFE0",
                 add.rug     = TRUE, speciesCol = "Sp1", recordDateTimeCol = "date",  recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", addLegend = FALSE)

activityOverlap (recordTable = aune, speciesA    = "vulpes vulpes" , speciesB = "sus scrofa", plotR = TRUE, linewidth = c(2.5, 2.5), 
                 linetype = c(1,1), linecol = c("#4682B4", "#D7C26F"), olapcol = "#FFFFE0",
                 add.rug     = TRUE, speciesCol = "Sp1", recordDateTimeCol = "date",  recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", addLegend = FALSE)

    #RUBUS (Sylvias & apodemus/rattus &cervus/genneta)
rulm%>%group_by(Sp1)%>%summarise(n())%>%data.frame()%>%arrange(desc(n..))

par(mfrow = c(2,2))
activityOverlap (recordTable = rulm, speciesA    = "sylvia melanocephala" , speciesB = "sylvia communis", plotR = TRUE, linewidth = c(2.5, 2.5), 
                 linetype = c(1,1), linecol = c("#4682B4", "#D7C26F"), olapcol = "#FFFFE0",
                 add.rug     = TRUE, speciesCol = "Sp1", recordDateTimeCol = "date",  recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", addLegend = FALSE)
activityOverlap (recordTable = rulm, speciesA    = "sylvia undata" , speciesB = "sylvia atricapilla", plotR = TRUE, linewidth = c(2.5, 2.5), 
                 linetype = c(1,1), linecol = c("#4682B4", "#D7C26F"), olapcol = "#FFFFE0",
                 add.rug     = TRUE, speciesCol = "Sp1", recordDateTimeCol = "date",  recordDateTimeFormat = "%Y-%m-%d %H:%M:%S",addLegend = FALSE)
activityOverlap (recordTable = rulm, speciesA    = "apode-mus" , speciesB = "rattus", plotR = TRUE, linewidth = c(2.5, 2.5), 
                 linetype = c(1,1), linecol = c("#4682B4", "#D7C26F"), olapcol = "#FFFFE0",
                 add.rug     = TRUE, speciesCol = "Sp1", recordDateTimeCol = "date",  recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", addLegend = FALSE)
activityOverlap (recordTable = rulm, speciesA    = "cervus elaphus" , speciesB = "genetta genetta", plotR = TRUE, linewidth = c(2.5, 2.5), 
                 linetype = c(1,1), linecol = c("#4682B4", "#D7C26F"), olapcol = "#FFFFE0",
                 add.rug     = TRUE, speciesCol = "Sp1", recordDateTimeCol = "date",  recordDateTimeFormat = "%Y-%m-%d %H:%M:%S",addLegend = FALSE)

      #Pyrus (fox/cervus & boar/badger)
pbou%>%group_by(Sp1)%>%summarise(n())%>%data.frame()%>%arrange(desc(n..))

par(mfrow = c(2,1))
activityOverlap (recordTable = pbou, speciesA    = "vulpes vulpes" , speciesB = "cervus elaphus", plotR = TRUE, linewidth = c(2.5, 2.5), 
                 linetype = c(1,1), linecol = c("#4682B4", "#D7C26F"), olapcol = "#FFFFE0",
                 add.rug     = TRUE, speciesCol = "Sp1", recordDateTimeCol = "date",  recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", addLegend = FALSE)
activityOverlap (recordTable = rulm, speciesA    = "sus scrofa" , speciesB = "meles meles", plotR = TRUE, linewidth = c(2.5, 2.5), 
                 linetype = c(1,1), linecol = c("#4682B4", "#D7C26F"), olapcol = "#FFFFE0",
                 add.rug     = TRUE, speciesCol = "Sp1", recordDateTimeCol = "date",  recordDateTimeFormat = "%Y-%m-%d %H:%M:%S",addLegend = FALSE)


      #Pistacia (sylvias and red robin)
plen%>%group_by(Sp1)%>%summarise(n())%>%data.frame()%>%arrange(desc(n..))

par(mfrow = c(2,2))
activityOverlap (recordTable = plen, speciesA    = "sylvia melanocephala" , speciesB = "sylvia communis", plotR = TRUE, linewidth = c(2.5, 2.5), 
                 linetype = c(1,1), linecol = c("#4682B4", "#D7C26F"), olapcol = "#FFFFE0",
                 add.rug     = TRUE, speciesCol = "Sp1", recordDateTimeCol = "date",  recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", addLegend = FALSE)
activityOverlap (recordTable = plen, speciesA    = "sylvia undata" , speciesB = "sylvia atricapilla", plotR = TRUE, linewidth = c(2.5, 2.5), 
                 linetype = c(1,1), linecol = c("#4682B4", "#D7C26F"), olapcol = "#FFFFE0",
                 add.rug     = TRUE, speciesCol = "Sp1", recordDateTimeCol = "date",  recordDateTimeFormat = "%Y-%m-%d %H:%M:%S",addLegend = FALSE)
activityOverlap (recordTable = plen, speciesA    = "erithacus rubecula" , speciesB = "chloris chloris", plotR = TRUE, linewidth = c(2.5, 2.5), 
                 linetype = c(1,1), linecol = c("#4682B4", "#D7C26F"), olapcol = "#FFFFE0",
                 add.rug     = TRUE, speciesCol = "Sp1", recordDateTimeCol = "date",  recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", addLegend = FALSE)

  #2.2 Zoocentric (by animal species)
    #2.2.1 Birds
act_data%>%group_by(Sp1)%>%summarise(n())%>%data.frame()%>%arrange(desc(n..))

par(mfrow = c(3,3))
activityDensity(recordTable = act_data, species = "sylvia melanocephala", speciesCol = "Sp1", recordDateTimeCol = "date", recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", plotR = TRUE)
activityDensity(recordTable = act_data, species = "turdus merula", speciesCol = "Sp1", recordDateTimeCol = "date", recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", plotR = TRUE)
activityDensity(recordTable = act_data, species = "sylvia communis", speciesCol = "Sp1", recordDateTimeCol = "date", recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", plotR = TRUE)
activityDensity(recordTable = act_data, species = "erithacus rubecula", speciesCol = "Sp1", recordDateTimeCol = "date", recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", plotR = TRUE)
activityDensity(recordTable = act_data, species = "turdus philomelos", speciesCol = "Sp1", recordDateTimeCol = "date", recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", plotR = TRUE)
activityDensity(recordTable = act_data, species = "sylvia atricapilla", speciesCol = "Sp1", recordDateTimeCol = "date", recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", plotR = TRUE)
activityDensity(recordTable = act_data, species = "chloris chloris", speciesCol = "Sp1", recordDateTimeCol = "date", recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", plotR = TRUE)
activityDensity(recordTable = act_data, species = "alectoris rufa", speciesCol = "Sp1", recordDateTimeCol = "date", recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", plotR = TRUE)
activityDensity(recordTable = act_data, species = "sylvia undata", speciesCol = "Sp1", recordDateTimeCol = "date", recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", plotR = TRUE)

    #2.2.2 Mammals
par(mfrow = c(3,3))
activityDensity(recordTable = act_data, species = "cervus elaphus", speciesCol = "Sp1", recordDateTimeCol = "date", recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", plotR = TRUE)
activityDensity(recordTable = act_data, species = "sus scrofa", speciesCol = "Sp1", recordDateTimeCol = "date", recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", plotR = TRUE)
activityDensity(recordTable = act_data, species = "vulpes vulpes", speciesCol = "Sp1", recordDateTimeCol = "date", recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", plotR = TRUE)
activityDensity(recordTable = act_data, species = "oryctolagus cuniculus", speciesCol = "Sp1", recordDateTimeCol = "date", recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", plotR = TRUE)
activityDensity(recordTable = act_data, species = "apode-mus", speciesCol = "Sp1", recordDateTimeCol = "date", recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", plotR = TRUE)
activityDensity(recordTable = act_data, species = "rattus", speciesCol = "Sp1", recordDateTimeCol = "date", recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", plotR = TRUE)
activityDensity(recordTable = act_data, species = "genetta genetta", speciesCol = "Sp1", recordDateTimeCol = "date", recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", plotR = TRUE)
activityDensity(recordTable = act_data, species = "meles meles", speciesCol = "Sp1", recordDateTimeCol = "date", recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", plotR = TRUE)
activityDensity(recordTable = act_data, species = "dama dama", speciesCol = "Sp1", recordDateTimeCol = "date", recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", plotR = TRUE)


  # 2.3 Overlap between two species 
activityOverlap (recordTable = act_data, speciesA    = "vulpes vulpes" , speciesB = "meles meles", plotR = TRUE, linewidth = c(2.5, 2.5), 
                 linetype = c(1,1), linecol = c("#4682B4", "#D7C26F"), olapcol = "#FFFFE0",
                 add.rug = TRUE, speciesCol = "Sp1", recordDateTimeCol = "date",  recordDateTimeFormat = "%Y-%m-%d %H:%M:%S")



#-------------------------- OTHER DANSITY PLOTS
# RUBUS most interesting (abundant birds) par of species 
par(mfrow = c(2,2))
activityOverlap (recordTable = rulm, speciesA    = "sylvia melanocephala" , speciesB = "sylvia communis", plotR = TRUE, linewidth = c(2.5, 2.5), 
                 linetype = c(1,1), linecol = c("#4682B4", "#D7C26F"), olapcol = "#FFFFE0",
                 add.rug     = TRUE, speciesCol = "Sp1", recordDateTimeCol = "date",  recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", addLegend = FALSE)

activityOverlap (recordTable = rulm, speciesA    = "sylvia undata" , speciesB = "sylvia atricapilla", plotR = TRUE, linewidth = c(2.5, 2.5), 
                 linetype = c(1,1), linecol = c("#4682B4", "#D7C26F"), olapcol = "#FFFFE0",
                 add.rug     = TRUE, speciesCol = "Sp1", recordDateTimeCol = "date",  recordDateTimeFormat = "%Y-%m-%d %H:%M:%S",addLegend = FALSE)

activityOverlap (recordTable = rulm, speciesA    = "sturnus unicolor" , speciesB = "hippolais poliglotta", plotR = TRUE, linewidth = c(2.5, 2.5), 
                 linetype = c(1,1), linecol = c("#4682B4", "#D7C26F"), olapcol = "#FFFFE0",
                 add.rug     = TRUE, speciesCol = "Sp1", recordDateTimeCol = "date",  recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", addLegend = FALSE)

activityOverlap (recordTable = rulm, speciesA    = "turdus merula" , speciesB = "saxicola rubicola", plotR = TRUE, linewidth = c(2.5, 2.5), 
                 linetype = c(1,1), linecol = c("#4682B4", "#D7C26F"), olapcol = "#FFFFE0",
                 add.rug     = TRUE, speciesCol = "Sp1", recordDateTimeCol = "date",  recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", addLegend = FALSE)

#Sylvia melanocephala foraging patterns in different plant species 

smel_rulm <- act_data %>%
  subset(Sp1 == "sylvia melanocephala") %>%
  filter(Plant== "rulm")

smel_aspa <- act_data %>%
  subset(Sp1 == "sylvia melanocephala") %>%
  filter(Plant== "aspa")

  

#Density plot for animal species    #####HEY!!! CHANGE THE DATES IN ASPA FOR SMEL from pm to am (change those between 21 h to 5:30 am -> pm)  
par(mfrow = c(2,1))

activityDensity(recordTable = smel_rulm, species = "sylvia melanocephala", speciesCol = "Sp1", recordDateTimeCol = "DateTime",
                recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", plotR = TRUE)

activityDensity(recordTable = smel_aspa, species = "sylvia melanocephala", speciesCol = "Sp1", recordDateTimeCol = "DateTime",
                recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", plotR = TRUE)


act_data %>%
  subset(Sp1 == "sylvia melanocephala") %>%
  group_by(Plant)%>%
  summarise(n())%>%
  data.frame()%>%
  arrange(desc(n..))

#Plot foraging time preference for each plant and animal species in a unique plot (compare same animal patterns in different plants)

#1. filter Plant species
aspa <- act_data %>%subset(Plant == "aspa")
aune <- act_data %>%subset(Plant == "aune")
calb <- act_data %>%subset(Plant == "calb")
joxy <- act_data %>%subset(Plant == "joxy")
mcom <- act_data %>%subset(Plant == "mcom")
oeur <- act_data %>%subset(Plant == "oeur")
olan <- act_data %>%subset(Plant == "olan")
pbou <- act_data %>%subset(Plant == "pbou")
rper <- act_data %>%subset(Plant == "rper")
rulm <- act_data %>%subset(Plant == "rulm")
sasp <- act_data %>%subset(Plant == "sasp")

#2. Plot each density plot by plant species
par(mfrow = c(2,3))
activityDensity(recordTable = aspa, species = "sylvia melanocephala", speciesCol = "Sp1", recordDateTimeCol = "date", recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", plotR = TRUE)
activityDensity(recordTable = aune, species = "sylvia melanocephala", speciesCol = "Sp1", recordDateTimeCol = "date", recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", plotR = TRUE)
activityDensity(recordTable = oeur, species = "sylvia melanocephala", speciesCol = "Sp1", recordDateTimeCol = "date", recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", plotR = TRUE)
activityDensity(recordTable = rulm, species = "sylvia melanocephala", speciesCol = "Sp1", recordDateTimeCol = "date", recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", plotR = TRUE)
activityDensity(recordTable = sasp, species = "sylvia melanocephala", speciesCol = "Sp1", recordDateTimeCol = "date", recordDateTimeFormat = "%Y-%m-%d %H:%M:%S", plotR = TRUE)

cervus <- act_data %>%
  filter(Sp1 == "cervus elaphus")%>%
  group_by(Plant)%>%
  summarise(n())%>%data.frame()%>%arrange(desc(n..))
```

## 4.6 RAREFACTION curves
```{r }
# code from Quintero et al 2022 Oikos paper - KK
#Summarise number of foraging species in individual plants
data8 <- data7 %>%
  group_by(Plant_ID, Sp1) %>%
  summarise(n_interactions=n())

#Transpose the dataframe into an observation matrix where each row is the data collection in each of the plant individuals. Columns are frugivore species
obs <- data8 %>%
  pivot_wider(names_from= Sp1, values_from= n_interactions, 
                     values_fill= list(count= 0)) %>%
  replace(is.na(.), 0)

#Extract sampling effort from data
samp_effort <- data7 %>%
  group_by(Plant_ID) %>%
  summarise(eff = first(effort_ind))

#Obtain Species Richness estimators:
specpool(obs[,-c(1:1)],obs$Plant_ID)

#PLOT
curve <- specaccum(obs[,-c(1:1)], method="random", permutations=100, effort=10, w=samp_effort$eff)

plot(curve$richness, ci.type="polygon",col="#555555", lwd=2, ci.lty=0, 
     ci.col= rgb(116/255,169/255,207/255,.6), ylab="Interaction richness", 
     xvar = "effort",
     xlab="Cameras working time in days", main="Camera Traps")

data7


 # Here we plot the species accumulation curve but using PAIRWISE INTERCTIONS instead of species and individual plants as sites.Here we plot an accumulation curve performed similarly as species diversity accumulation curve analysis considering each pairwise-interaction as a “species” and the different Plant individuals as sampling units (Jordano 2016). 

#Summarise number of foraging species in individual plants
data8 <- data7 %>%
   mutate(pl = str_sub(Plant_ID, start = 1L, end = 4),
         pairwise_int = str_c(pl, Sp1, sep = "_"))%>%
  group_by(Plant_ID, pairwise_int) %>%
  summarise(n_interactions=n())
  
#Transpose the dataframe into an observation matrix where each row is the data collection in each of the plant individuals. Columns are frugivore species
obs <- data8 %>%
  pivot_wider(names_from= pairwise_int, values_from= n_interactions, 
                     values_fill= list(count= 0)) %>%
  replace(is.na(.), 0) 

sp2<-specaccum(obs[,-c(1:3)])

plot(sp2, ci.type="poly", col="blue", lwd=2, ci.lty=0, ci.col="lightblue",
      xlab = "number of plant individuals", ylab = "interaction richness")



#Obtain Species Richness estimators (Species richness/Plant_ID):
specpool(obs[,-c(1:3)],obs$Plant_ID)
 
 
 
```

## 4.7 PHENOLOGY density plots
```{r }
#Beware of TImestamp ISSUES (eliminate them)
    
    #1.Create a df
    date <- as.Date(data7$DateTime)
    week <- format(date, "%V")
    plant_sp <- data7$Plant
    interaction <- as.integer(!is.na(data7$Sp1))
      d <- data.frame(cbind(week, plant_sp, interaction))
    
    #crear la tabla para plotear (contar el nuemro de interacciones por semana)
    uncount.f.data <- as.data.frame(d %>% 
                                      group_by(week, plant_sp) %>%
                                      summarise(sum = sum(as.numeric(interaction), na.rm=TRUE))) # contar numero de interacciones (sólo eating) agrupadas (sum) por semana 
    uncount.f.data <- tidyr::uncount(na.omit(uncount.f.data), weights=as.integer(sum))
    
    #Crear un vector para ordenar el eje de la x por semanas desde que se comenzó el muestreo
    week_order <- c("28", "29", "30", "31", "32", "33", "34", "35" ,"36", "37" ,"38" ,"39", "40" ,"41", "42" ,"43" ,"44" ,"45", "46", "47", "48", "49", "50", "51", "52", "01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20","21","22","23","24","25","26","27")  
    
    
    #Plotear los density plots por especie de planta
    ggplot(uncount.f.data, aes(x = factor(week, level = week_order), y = plant_sp, group = plant_sp, fill = plant_sp)) + 
      geom_violin(bw=0.8, alpha=0.7) + theme_classic() 
    

      
      ####################################
      ####################################
      #                                  #
      #           ZOOCENTRIC             #
      #                                  #
      ####################################
      ####################################
      
      #1. crear el dataset
        sp1 <- subset(data7, Sp1 == "vulpes vulpes")
      
      date <- as.Date(data7$DateTime)
      week <- format(date, "%V")
      animal <- data7$Sp1 
      interaction <- as.integer(!is.na(data7$Sp1))
    
      
      d <- data.frame(cbind(week, animal, interaction))
    
      
      #crear la tabla para plotear (contar el nuemro de interacciones por semana)
      uncount.f.data <- as.data.frame(d %>% 
                                        group_by(week, animal) %>%
                                        summarise(sum = sum(as.numeric(interaction), na.rm=TRUE))) # contar numero de interacciones (sólo eating) agrupadas (sum) por semana 
      uncount.f.data <- tidyr::uncount(na.omit(uncount.f.data), weights=as.integer(sum))
      
      #Plotear los density plots por especie de planta
      ggplot(uncount.f.data, aes(x = week, y = animal, group = animal, fill = animal)) + 
        geom_violin(bw=0.8, alpha=0.7) + theme_classic() 
      
      theme(legend.position = "none", axis.text.y = element_text(face="bold", size=13),
            axis.text.x = element_text(face="bold", size=13),
            axis.title.x = element_text(face="bold", size=15, 
                                        margin = margin(t = 20, r = 0, b = 0, l = 0)),
            axis.title.y = element_blank())   
    
   #Fechas de colocación del primer deployment en el yr1    
      Aune_date <- format(as.Date("26/10/21"), "%V")
      Aspa_date <- format(as.Date("18/11/21"), "%V")
      Mcom_date <- format(as.Date("14/10/21"), "%V")
      Oeur_date <- format(as.Date("26/10/21"), "%V")
      Ppin_date <- format(as.Date("8/10/21"), "%V")
      Pbou_date <- format(as.Date("27/9/21"), "%V")
      Rper_date <- format(as.Date("14/10/21"), "%V")
      Rulm_date <- format(as.Date("19/8/21"), "%V")
      Sasp_date <- format(as.Date("21/10/21"), "%V")
      Joxy_date <- format(as.Date("16/12/21"), "%V")
      Corema2_date <- format(as.Date("15/7/21"), "%V")
      
      
      
```



